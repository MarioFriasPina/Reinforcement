{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilhui\\AppData\\Local\\Temp\\ipykernel_3384\\3808058653.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensors = [torch.tensor(s, dtype=torch.float32) for s in state]\n",
      "C:\\Users\\ilhui\\AppData\\Local\\Temp\\ipykernel_3384\\3808058653.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  memory['actions'].append(torch.tensor(actions[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: tensor([-6.8778])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (800x18 and 128x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 129\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_rewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Actualizar la política al final del episodio\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(memory)\n\u001b[0;32m    130\u001b[0m memory\u001b[38;5;241m.\u001b[39mclear()\n",
      "Cell \u001b[1;32mIn[9], line 60\u001b[0m, in \u001b[0;36mPPOAgent.update\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m     58\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(memory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     59\u001b[0m masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(memory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 60\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mcritic(states)\n\u001b[0;32m     61\u001b[0m returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_returns(rewards, masks, values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m     63\u001b[0m advantages \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;241m-\u001b[39m values\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (800x18 and 128x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import vmas\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Definición de la política PPO\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(PPOPolicy, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def act(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action)\n",
    "\n",
    "    def evaluate(self, x, actions):\n",
    "        logits, values = self.forward(x)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action_logprobs = dist.log_prob(actions)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, values, dist_entropy\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, clip_eps=0.2, k_epochs=4):\n",
    "        self.policy = PPOPolicy(input_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_eps = clip_eps\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "    def compute_returns(self, rewards, masks, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * masks[step]\n",
    "            returns.insert(0, R)\n",
    "        return torch.cat(returns)\n",
    "\n",
    "    def update(self, memory):\n",
    "        states = torch.stack(memory['states']).squeeze(1)\n",
    "        actions = torch.cat(memory['actions'])\n",
    "        log_probs_old = torch.cat(memory['log_probs'])\n",
    "        rewards = torch.cat(memory['rewards'])\n",
    "        masks = torch.cat(memory['masks'])\n",
    "        values = self.policy.critic(states)\n",
    "        returns = self.compute_returns(rewards, masks, values[-1].detach())\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "        for _ in range(self.k_epochs):\n",
    "            log_probs, values, dist_entropy = self.policy.evaluate(states, actions)\n",
    "            ratio = torch.exp(log_probs - log_probs_old.detach())\n",
    "\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.MSELoss()(returns, values)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + 0.5 * value_loss - 0.01 * dist_entropy.mean()).backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "# Inicializar el entorno `navigation`\n",
    "env = vmas.make_env(\n",
    "    scenario=\"navigation\",\n",
    "    num_envs=1,\n",
    "    device=\"cpu\",  # Cambia a \"cuda\" si tienes GPU disponible\n",
    "    continuous_actions=False,\n",
    "    max_steps=200,\n",
    ")\n",
    "\n",
    "# Dimensiones de observaciones y acciones\n",
    "input_dim = env.observation_space[0].shape[0]\n",
    "action_dim = env.action_space[0].n\n",
    "\n",
    "# Crear el agente PPO\n",
    "agent = PPOAgent(input_dim, action_dim)\n",
    "\n",
    "# Entrenamiento PPO\n",
    "max_episodes = 500\n",
    "memory = defaultdict(list)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    episode_rewards = 0\n",
    "    for _ in range(200):  # Máximo de pasos por episodio\n",
    "        state_tensors = [torch.tensor(s, dtype=torch.float32) for s in state]\n",
    "\n",
    "        actions, log_probs = [], []\n",
    "        for state_tensor in state_tensors:\n",
    "            action, log_prob = agent.policy.act(state_tensor)\n",
    "            actions.append(action.numpy())  # Acción individual para cada agente\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            memory['states'].append(state_tensors[i])\n",
    "            memory['actions'].append(torch.tensor(actions[i]))\n",
    "            memory['log_probs'].append(log_probs[i])\n",
    "            memory['rewards'].append(torch.tensor([reward[i]], dtype=torch.float32))  # Aseguramos 1D\n",
    "            # Convertir `done` en tensor de al menos una dimensión\n",
    "            done_value = done[i] if isinstance(done, list) else done\n",
    "            memory['masks'].append(torch.tensor([1.0 if not done_value else 0.0], dtype=torch.float32))\n",
    "\n",
    "        state = next_state\n",
    "        episode_rewards += sum(reward)  # Sumar recompensas de todos los agentes\n",
    "        if all(done):  # Terminar el episodio si todos los agentes están listos\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode} Reward: {episode_rewards}\")\n",
    "\n",
    "    # Actualizar la política al final del episodio\n",
    "    agent.update(memory)\n",
    "    memory.clear()\n",
    "\n",
    "\n",
    "\n",
    "# Renderizar el entorno con la política aprendida\n",
    "# for _ in range(10):  # Visualizar 10 episodios\n",
    "#     state = env.reset()\n",
    "#     total_rewards = 0\n",
    "#     for _ in range(200):  # Máximo de pasos por episodio\n",
    "#         state_tensor = torch.tensor(state[0], dtype=torch.float32)\n",
    "#         action, _ = agent.policy.act(state_tensor)  # Usar la política entrenada\n",
    "#         action_list = [action.numpy()]  # Convertir la acción a lista para VMAS\n",
    "#         next_state, reward, done, _ = env.step(action_list)\n",
    "\n",
    "#         total_rewards += reward[0]\n",
    "#         env.render(mode=\"human\")  # Renderizar el episodio\n",
    "\n",
    "#         if done[0]:\n",
    "#             break\n",
    "\n",
    "#         state = next_state\n",
    "\n",
    "#     print(f\"Total Reward for Episode: {total_rewards}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
