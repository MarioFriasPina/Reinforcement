{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n",
      "Observation spaces: {'prey_0': 16}\n",
      "Action spaces: {'prey_0': 5}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "args = {\n",
    "    'render_mode': None,\n",
    "    'max_cycles': 256,\n",
    "    'continuous_actions': True,\n",
    "    'num_predators': 0,\n",
    "    'num_prey': 1,\n",
    "    'num_obstacles': 0,\n",
    "    'num_food': 1,\n",
    "    'num_water': 1,\n",
    "    'num_forests': 0\n",
    "}\n",
    "\n",
    "env = natural_env_v0.parallel_env(**args)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Get observation and action spaces\n",
    "obs_spaces = {agent: env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_spaces = {agent: env.action_space(agent).shape[0] for agent in env.agents}\n",
    "agents = env.agents  # List of agents\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"Observation spaces: {obs_spaces}\")\n",
    "print(f\"Action spaces: {action_spaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        input_dim = obs_dim + action_dim  # Critic gets all obs & actions\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs, actions):\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "class ParallelReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, observations, actions, rewards, next_observations, dones):\n",
    "        # Store a single transition for all agents\n",
    "        self.buffer.append((observations, actions, rewards, next_observations, dones))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert dictionary data into tensors\n",
    "        obs = {agent: torch.tensor(np.array([b[agent] for b in obs]), dtype=torch.float32, device=device) for agent in obs[0]}\n",
    "        actions = {agent: torch.tensor(np.array([b[agent] for b in actions]), dtype=torch.float32, device=device) for agent in actions[0]}\n",
    "        rewards = {agent: torch.tensor([b[agent] for b in rewards], dtype=torch.float32, device=device) for agent in rewards[0]}\n",
    "        next_obs = {agent: torch.tensor(np.array([b[agent] for b in next_obs]), dtype=torch.float32, device=device) for agent in next_obs[0]}\n",
    "        dones = {agent: torch.tensor([b[agent] for b in dones], dtype=torch.float32, device=device) for agent in dones[0]}\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(actors, critics, target_actors, target_critics, buffer, batch_size, gamma=0.95, tau=0.01):\n",
    "    obs, actions, rewards, next_obs, dones = buffer.sample(batch_size)\n",
    "    \n",
    "    # Centralized Q-value update for each agent\n",
    "    for i, agent in enumerate(agents):\n",
    "        # Get target actions for all agents\n",
    "        target_actions = torch.cat([target_actors[j](next_obs[other]) for j, other in enumerate(agents)], dim=-1)\n",
    "        obs_concat = torch.cat([obs[other] for other in agents], dim=-1)\n",
    "        next_obs_concat = torch.cat([next_obs[other] for other in agents], dim=-1)\n",
    "        \n",
    "        # Compute target Q-value\n",
    "        target_q = target_critics[i](next_obs_concat, target_actions).detach()\n",
    "        y = rewards[agent] + gamma * (1 - dones[agent]) * target_q.squeeze()\n",
    "        \n",
    "        # Predicted Q-value\n",
    "        actions_concat = torch.cat([actions[other] for other in agents], dim=-1)\n",
    "        current_q = critics[i](obs_concat, actions_concat).squeeze()\n",
    "        \n",
    "        # Critic Loss\n",
    "        critic_loss = torch.nn.functional.mse_loss(current_q, y)\n",
    "        critics[i].optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critics[i].optimizer.step()\n",
    "\n",
    "    # Policy (actor) update\n",
    "    for i, agent in enumerate(agents):\n",
    "        current_actions = torch.cat(\n",
    "            [actors[j](obs[other]) if other == agent else actions[other].detach() for j, other in enumerate(agents)], dim=-1\n",
    "        )\n",
    "        actor_loss = -critics[i](obs_concat, current_actions).mean()\n",
    "        actors[i].optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actors[i].optimizer.step()\n",
    "\n",
    "    # Soft update for target networks\n",
    "    for i, agent in enumerate(agents):\n",
    "        for target_param, param in zip(target_critics[i].parameters(), critics[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(target_actors[i].parameters(), actors[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "def update_plot_multi_agent(episode, max_episodes, reward_history, show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(f'Final Result:')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "\n",
    "    for agent_name in agents:\n",
    "        agent_name = agents[0]\n",
    "        rewards_t = torch.tensor(reward_history[agent_name], dtype=torch.float)\n",
    "        plt.plot(rewards_t.numpy(), label=agent_name)\n",
    "\n",
    "        # Plot moving average of last 10 rewards\n",
    "        if len(rewards_t) >= 10:\n",
    "            means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            plt.plot(means.numpy())            \n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Received an action [-0.99759626 -1.         -1.         -1.          0.9999999 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.990686  -1.        -1.        -1.         0.9999985] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.97165203 -1.         -1.         -0.9999998   0.9999912 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.9249356  -1.         -1.         -0.99999624  0.99995196] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.88830984 -1.         -1.         -0.99992234  0.9997748 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.80804425 -1.         -1.         -0.9983775   0.998819  ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [-0.3890307 -1.        -1.        -0.9683111  0.9897664] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.28596553 -1.         -1.         -0.56991154  0.8868846 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.73837    -1.         -1.          0.6633393   0.18174855] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.81276923 -1.         -1.          0.27004474  0.23566392] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.8339347  -1.         -1.          0.4549369  -0.13227001] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.82066697 -1.         -1.          0.27629825 -0.23725319] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.869209  -1.        -1.         0.5912549 -0.5957348] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.8621571  -1.         -1.          0.5260834  -0.63263875] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.8625599 -1.        -1.         0.5883339 -0.6770017] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.78921974 -1.         -1.          0.4563726  -0.6023061 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.7645834  -1.         -1.          0.5805646  -0.68065196] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.7011147  -1.         -1.          0.57868093 -0.663693  ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.7011205  -1.         -1.          0.61226773 -0.6592001 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.65559703 -1.         -1.          0.6564647  -0.62521535] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.57587576 -0.9999999  -1.          0.548863   -0.52009696] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.5334835  -0.99999875 -1.          0.59220433 -0.48508722] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.546136   -0.99998933 -1.          0.6905261  -0.49954635] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.5738338  -0.99989486 -0.99999887  0.8543356  -0.5850171 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.62654734 -0.9990469  -0.9999588   0.941695   -0.7065572 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.7934013  -0.99222225 -0.9995578   0.98111373 -0.8620479 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.8808998  -0.9810801  -0.9990871   0.98506767 -0.8958136 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.9498817  -0.973739   -0.9992238   0.9885197  -0.91862565] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.9835413  -0.96533203 -0.9995352   0.9931522  -0.9401559 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.99399626 -0.96799505 -0.9997722   0.9961559  -0.94942856] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.99788815 -0.97732264 -0.99991286  0.9981906  -0.9559343 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.99897134 -0.9886153  -0.9999688   0.99936974 -0.9414634 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [ 0.9988201  -0.9968031  -0.9999881   0.99981105 -0.8421    ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Training step if enough data in buffer\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 50\u001b[0m         \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_actors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_critics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m {agent_name: reward_history[agent_name] \u001b[38;5;241m+\u001b[39m [episode_reward[agent_name]] \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m     53\u001b[0m update_plot_multi_agent(episode, episodes, reward_history)\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(actors, critics, target_actors, target_critics, buffer, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m     30\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcritics[i](obs_concat, current_actions)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     31\u001b[0m     actors[i]\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     actors[i]\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Soft update for target networks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/torch/_tensor.py:571\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m             Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m             (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m             inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m         )\n\u001b[1;32m    581\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize actors, critics, target networks, and optimizers\n",
    "actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "target_actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "target_critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "\n",
    "# Optimizers\n",
    "for actor, critic in zip(actors, critics):\n",
    "    actor.optimizer = Adam(actor.parameters(), lr=1e-3)\n",
    "    critic.optimizer = Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = ParallelReplayBuffer()\n",
    "\n",
    "# Main training loop\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "plt.ion()\n",
    "reward_history = {agent_name: [] for agent_name in agents}\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Get initial observations\n",
    "    done = defaultdict(bool, {agent: False for agent in agents})\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "    \n",
    "    while not all(done.values()):\n",
    "        # Choose actions for each agent\n",
    "        actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, rewards, terminated, truncated,  _ = env.step(actions)\n",
    "\n",
    "        # Compute reward for each agent\n",
    "        for agent in agents:\n",
    "            episode_reward[agent] += rewards[agent]\n",
    "\n",
    "        # Update done flag\n",
    "        done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "                \n",
    "        # Store transition in replay buffer\n",
    "        buffer.add(obs, actions, rewards, next_obs, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        # Training step if enough data in buffer\n",
    "        if buffer.size() > batch_size:\n",
    "            train_step(actors, critics, target_actors, target_critics, buffer, batch_size)\n",
    "\n",
    "    reward_history = {agent_name: reward_history[agent_name] + [episode_reward[agent_name]] for agent_name in agents}\n",
    "    update_plot_multi_agent(episode, episodes, reward_history)\n",
    "\n",
    "update_plot_multi_agent(episode + 1, episodes, reward_history, show_result=True)\n",
    "plt.ioff()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "human_env = natural_env_v0.parallel_env(render_mode=\"human\", continuous_actions=True, **args)\n",
    "\n",
    "obs, _ = human_env.reset()\n",
    "done = defaultdict(bool, {agent: False for agent in agents})\n",
    "\n",
    "while not all(done.values()):\n",
    "    actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "    obs, _, terminated, truncated, _ = human_env.step(actions)\n",
    "    done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "\n",
    "human_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
