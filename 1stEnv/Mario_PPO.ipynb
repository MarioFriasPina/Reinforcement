{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n",
      "Observation spaces: {'adversary_0': 16, 'adversary_1': 16, 'adversary_2': 16, 'agent_0': 14}\n",
      "Action spaces: {'adversary_0': np.int64(5), 'adversary_1': np.int64(5), 'adversary_2': np.int64(5), 'agent_0': np.int64(5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfrias/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 1: invalid argument (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from NaturalEnv import natural_env_v0\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = {\n",
    "    'render_mode': None,\n",
    "    'max_cycles': 256,\n",
    "    'continuous_actions': False,\n",
    "    'num_predators': 0,\n",
    "    'num_prey': 1,\n",
    "    'num_obstacles': 0,\n",
    "    'num_food': 1,\n",
    "    'num_water': 1,\n",
    "    'num_forests': 0\n",
    "}\n",
    "\n",
    "env = simple_tag_v3.parallel_env(max_cycles=128, continuous_actions=False)\n",
    "#env = natural_env_v0.parallel_env(**args)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Get observation and action spaces\n",
    "obs_spaces = {agent: env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_spaces = {agent: env.action_space(agent).n for agent in env.agents}\n",
    "agents = env.agents  # List of agents\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"Observation spaces: {obs_spaces}\")\n",
    "print(f\"Action spaces: {action_spaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Policy and Value Function\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Shared layers between policy and value\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Policy function for PPO\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Value function for PPO\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        x = self.shared(obs)\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "# PPO Algorithm\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim, lr=3e-4, gamma=0.99, clip_epsilon=0.2, update_steps=4, batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.update_steps = update_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor_critic = ActorCritic(obs_dim, act_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "    \n",
    "    def compute_returns(self, rewards, dones, last_value):\n",
    "        \"\"\"\n",
    "        Calculates the expected cumulative rewards for each time step\n",
    "\n",
    "        Args:\n",
    "            rewards (list): List of rewards for each time step\n",
    "            dones (list): List of done flags for each time step\n",
    "            last_value (float): Value estimate for the last time step\n",
    "\n",
    "        Returns:\n",
    "            returns (list): Expected cumulative rewards for each time step\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        R = last_value\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + self.gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "    \n",
    "    def update(self, obs, actions, old_log_probs, returns, advantages):\n",
    "        \"\"\"\n",
    "        Updates the policy and value function using PPO\n",
    "\n",
    "        Args:\n",
    "            obs (list): List of observations for each time step\n",
    "            actions (list): List of actions for each time step\n",
    "            old_log_probs (list): List of old log probabilities for each time step\n",
    "            returns (list): List of expected cumulative rewards for each time step\n",
    "            advantages (list): List of advantages for each time step\n",
    "        \"\"\"\n",
    "        # Convert to tensors, move to device. Everything that was not already on the device\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32, device=device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "\n",
    "        for _ in range(self.update_steps):\n",
    "            for start in range(0, len(obs), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_obs = obs[start:end]\n",
    "                batch_actions = actions[start:end]\n",
    "                batch_old_log_probs = old_log_probs[start:end]\n",
    "                batch_returns = returns[start:end]\n",
    "                batch_advantages = advantages[start:end]\n",
    "\n",
    "                # Forward pass\n",
    "                policy, values = self.actor_critic(batch_obs)\n",
    "                dist = Categorical(policy)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # PPO loss\n",
    "                ratios = (new_log_probs - batch_old_log_probs).exp()\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = ((batch_returns - values.squeeze()) ** 2).mean()\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot_multi_agent(episode, max_episodes, reward_history, show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(f'Final Result:')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "\n",
    "    for agent_name in agents:\n",
    "        rewards_t = torch.tensor(reward_history[agent_name], dtype=torch.float)\n",
    "        plt.plot(rewards_t.numpy(), label=agent_name)\n",
    "\n",
    "        \"\"\"\n",
    "        # Plot moving average of last 10 rewards\n",
    "        if len(rewards_t) >= 10:\n",
    "            means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            plt.plot(means.numpy())\n",
    "        \"\"\"\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize PPO agents\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m actors \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43magent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_spaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_spaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Epsilon Greedy Exploration\u001b[39;00m\n\u001b[1;32m      5\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize PPO agents\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m actors \u001b[38;5;241m=\u001b[39m {agent: \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_spaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_spaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Epsilon Greedy Exploration\u001b[39;00m\n\u001b[1;32m      5\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPO' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Initialize PPO agents\n",
    "actors = {agent: PPO(obs_spaces[agent], action_spaces[agent]).to(device) for agent in agents}\n",
    "\n",
    "# Epsilon Greedy Exploration\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "\n",
    "# Main training loop\n",
    "episodes = 100\n",
    "\n",
    "plt.ion()\n",
    "reward_history = {agent_name: [] for agent_name in agents}\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Get initial observations\n",
    "    done = defaultdict(bool, {agent: False for agent in agents})\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "    \n",
    "    while not all(done.values()):\n",
    "        # Get actions for each agent\n",
    "        actions, log_probs, values = {}, {}, {}\n",
    "\n",
    "        # Convert observations to tensors\n",
    "        states = {agent: torch.tensor(obs[agent], dtype=torch.float32, device=device).unsqueeze(0) for agent in agents}\n",
    "\n",
    "        for i, agent in enumerate(agents):\n",
    "            # Get action probabilities from actor\n",
    "            policy, value = actors[agent].actor_critic(states[agent])\n",
    "            dist = Categorical(policy)\n",
    "            action = dist.sample().item()\n",
    "            actions[agent] = action\n",
    "\n",
    "            # Store log probabilities and values\n",
    "            log_probs[agent] = dist.log_prob(torch.tensor([action], dtype=torch.float32)).item()\n",
    "            values[agent] = value.item()\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, rewards, terminated, truncated,  _ = env.step(actions)\n",
    "\n",
    "        # Update done flag\n",
    "        done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "\n",
    "        for agent in agents:\n",
    "            # Compute reward for each agent\n",
    "            episode_reward[agent] += rewards[agent]\n",
    "\n",
    "            # Update PPO for each agent\n",
    "            if not done[agent]:\n",
    "                last_value = values[agent]\n",
    "                returns = actors[agent].compute_returns([rewards[agent]], [done[agent]], last_value)\n",
    "                advantages = np.array(returns) - values[agent]\n",
    "\n",
    "                actors[agent].update(states[agent], [actions[agent]], [log_probs[agent]], returns, advantages)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "    # Update reward history\n",
    "    reward_history = {agent_name: reward_history[agent_name] + [episode_reward[agent_name]] for agent_name in agents}\n",
    "    update_plot_multi_agent(episode, episodes, reward_history)\n",
    "\n",
    "update_plot_multi_agent(episode + 1, episodes, reward_history, show_result=True)\n",
    "plt.ioff()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "args['render_mode'] = 'human'\n",
    "#human_env = natural_env_v0.parallel_env(**args)\n",
    "human_env = simple_tag_v3.parallel_env(render_mode=\"human\", continuous_actions=False)\n",
    "\n",
    "obs, _ = human_env.reset()\n",
    "\n",
    "done = defaultdict(bool, {agent: False for agent in agents})\n",
    "\n",
    "while not all(done.values()):\n",
    "    actions = {}\n",
    "    for agent in agents:\n",
    "        state = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0)\n",
    "        policy, _ = actors[agent].actor_critic(state)\n",
    "        dist = Categorical(policy)\n",
    "        action = dist.sample().item()\n",
    "        actions[agent] = action\n",
    "\n",
    "    obs, _, terminated, truncated, _ = human_env.step(actions)\n",
    "\n",
    "    done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "\n",
    "human_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
