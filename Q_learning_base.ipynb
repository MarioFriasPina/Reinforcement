{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "# Inicializar el entorno\n",
    "env = natural_env_v0.parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "action_space = 5\n",
    "epsilon = 0.5\n",
    "\n",
    "# Inicializar modelos de Q-learning para cada agente\n",
    "model_params = {agent: np.zeros((observations[agent].shape[0], action_space)) for agent in env.agents}\n",
    "\n",
    "# Funciones de ayuda\n",
    "def calc_state(observation):\n",
    "    return int(observation.sum()) % observation.shape[0]\n",
    "\n",
    "def get_action(state, model_params, exploration=True):\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    else:\n",
    "        return model_params[state].argmax()\n",
    "\n",
    "def train(experience, model_params, lr=learning_rate, df=discount_factor):\n",
    "    \"\"\"Entrena el modelo actualizando parámetros usando Q-learning.\"\"\"\n",
    "    for prev_state, action_taken, state, reward in reversed(experience):\n",
    "        best_next_action = model_params[state].max()\n",
    "        target = reward + df * best_next_action\n",
    "        model_params[prev_state][action_taken] += lr * (target - model_params[prev_state][action_taken])\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "    observations, infos = env.reset()\n",
    "    experience = {agent: [] for agent in env.agents}\n",
    "    total_rewards = {agent: 0 for agent in env.agents}\n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while env.agents:\n",
    "        # Seleccionar acciones para cada agente\n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            state = calc_state(observations[agent])\n",
    "            action = get_action(state, model_params[agent])\n",
    "            actions[agent] = action\n",
    "\n",
    "        # Tomar un paso en el entorno\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Registrar experiencia y actualizar recompensas acumuladas\n",
    "        for agent in env.agents:\n",
    "            state = calc_state(observations[agent])\n",
    "            prev_state = calc_state(observations[agent])\n",
    "            action = actions[agent]\n",
    "            reward = rewards[agent]\n",
    "            total_rewards[agent] += reward\n",
    "\n",
    "            experience[agent].append([prev_state, action, state, reward])\n",
    "\n",
    "            # Verificar si el agente ha terminado\n",
    "            if terminations[agent] or truncations[agent]:\n",
    "                env.agents.remove(agent)\n",
    "\n",
    "    # Entrenar al final del episodio\n",
    "    for agent in experience:\n",
    "        train(experience[agent], model_params[agent])\n",
    "\n",
    "    # Mostrar las recompensas acumuladas\n",
    "    print(f\"Recompensas acumuladas: {total_rewards}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
