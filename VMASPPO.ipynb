{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "is_fork = multiprocessing.get_start_method() == 'fork'\n",
    "device = (\n",
    "    torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device('cpu')\n",
    ")\n",
    "vmas_device = device\n",
    "\n",
    "# Epsilon-greedy\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.999  # Exploration decay rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lr_agents = 0.001  # learning rate for agents\n",
    "lr_critic = 0.001  # learning rate for critic\n",
    "\n",
    "# Environment\n",
    "scenario_name = \"dropout\"\n",
    "\n",
    "num_epochs = 1024  # Total number of training epochs, i.e. number of steps\n",
    "max_steps = 128  # Episode steps before reset\n",
    "num_vmas_envs = 1  # Number of vectorized environments to simulate at once\n",
    "\n",
    "n_agents = 1 # Number of agents in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action spaces: Dict('agent_0': Discrete(9))\n",
      "Observation spaces: Dict('agent_0': Box(-inf, inf, (7,), float32))\n",
      "Action size: 9\n",
      "Observation size: 7\n"
     ]
    }
   ],
   "source": [
    "env = vmas.make_env(\n",
    "    scenario=scenario_name, # Name of the scenario\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=False,  # Use discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    dict_spaces=True, # Use a dictionary for the observation and action spaces, instead of a tuple\n",
    "\n",
    "    # Scenario custom args\n",
    "    n_agents=n_agents,\n",
    ")\n",
    "\n",
    "print(\"Action spaces:\", env.action_space)\n",
    "print(\"Observation spaces:\", env.observation_space)\n",
    "\n",
    "# Get list of agents\n",
    "agents = [agent.name for agent in env.agents]\n",
    "\n",
    "# Get the name of the first agent\n",
    "first_agent = agents[0]\n",
    "\n",
    "# Obtain the size of the action space\n",
    "action_size = env.action_space[first_agent].n\n",
    "\n",
    "# Obtain the size of the observation space\n",
    "observation_size = env.observation_space[first_agent].shape[0]\n",
    "\n",
    "print(\"Action size:\", action_size)\n",
    "print(\"Observation size:\", observation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Network and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_size, observation_size, hidden_size=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_agents)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, observation_size, hidden_size=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_critic)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# PPO algorithm\n",
    "def update(experience, actor, critic, updates_per_iter=5, df=gamma, eps=clip_epsilon):\n",
    "    # Initialize rewards to go\n",
    "    rtgs = 0\n",
    "    # Unpack experience\n",
    "    for observations, actions, rewards, dones in reversed(experience):\n",
    "        # Compute rewards to go\n",
    "        rtgs = rtgs * df + rewards\n",
    "\n",
    "        # Save previous policy\n",
    "        prev_policy = actor(observations).detach()\n",
    "\n",
    "        # Update policy\n",
    "        for i in range(updates_per_iter):\n",
    "            # Compute advantage estimation\n",
    "            advantage = rtgs - critic(observations)\n",
    "\n",
    "            # Compute ratio between new and old policy, i.e. pi(a|s) / pi_old(a|s)\n",
    "            ratio = torch.exp(torch.log(prev_policy[actions]) - torch.log(actor(observations)))\n",
    "\n",
    "            # Compute clipped ratio\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "\n",
    "            # Update policy via stochastic gradient descent (Adam)\n",
    "            actor.optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor.optimizer.step()\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = torch.mean((rtgs - critic(observations)) ** 2)\n",
    "\n",
    "            # Update critic via stochastic gradient descent (Adam)\n",
    "            critic.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic.optimizer.step()\n",
    "\n",
    "def get_action(obs, actor):\n",
    "    # Get actions for each simultaneous environment\n",
    "    actions = []\n",
    "    for o in obs:\n",
    "        actions.append(actor(o).argmax().item())\n",
    "\n",
    "    return torch.tensor(actions)\n",
    "\n",
    "# Create PPO agents\n",
    "agent_policy = Actor(action_size, observation_size).to(device)\n",
    "critic = Critic(observation_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot_multi_agent(episode, max_episodes, reward_history, show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(f'Final Result:')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Mean reward')\n",
    "\n",
    "    plt.plot(reward_history)\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "reward_means = []\n",
    "\n",
    "# Initialize the environment\n",
    "observations = env.reset()\n",
    "\n",
    "# Save the experience of each agent for use in PPO training\n",
    "experience = [{agent.name: [] for agent in env.agents} for _ in range(num_vmas_envs)]\n",
    "\n",
    "# Train for num_epochs\n",
    "for e in range(1, num_epochs):\n",
    "\n",
    "    # Epsilon-greedy exploration\n",
    "    if epsilon > np.random.rand():\n",
    "        actions = {agent.name: env.get_random_action(agent) for agent in env.agents}\n",
    "    else:\n",
    "        # Get actions for each agent\n",
    "        actions = {agent.name: get_action(observations[agent.name], agent_policy) for agent in env.agents}\n",
    "\n",
    "    # Step the environment\n",
    "    observations, rewards, done, info = env.step(actions)\n",
    "\n",
    "    # Save the experience of each agent\n",
    "    for env_idx in range(num_vmas_envs):\n",
    "        for agent in env.agents:\n",
    "            experience[env_idx][agent.name].append((observations[agent.name][env_idx], actions[agent.name][env_idx], rewards[agent.name][env_idx], done[env_idx]))        \n",
    "\n",
    "    # Reset the environment if done\n",
    "    for i in range(num_vmas_envs):\n",
    "        if done[i]:\n",
    "\n",
    "            # Update the PPO\n",
    "            for agent in env.agents:\n",
    "                update(experience[i][agent.name], agent_policy, critic)\n",
    "\n",
    "            # Reset the environment\n",
    "            observations = env.reset_at(i)\n",
    "            experience[i] = {agent.name: [] for agent in env.agents}\n",
    "\n",
    "    # Get the average reward of all simulations, this assumes that all agents have the same reward\n",
    "    reward_means.append(sum(rewards[first_agent]) / num_vmas_envs)\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # Update plot\n",
    "    update_plot_multi_agent(e, num_epochs, reward_means)\n",
    "\n",
    "update_plot_multi_agent(e + 1, num_epochs, reward_means, show_result=True)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()\n",
    "done = [False]\n",
    "\n",
    "while not done[0]:\n",
    "    #actions = {agent.name: env.get_random_action(agent) for agent in env.agents}\n",
    "    actions = {agent.name: get_action(observations[agent.name], agent_policy) for agent in env.agents}\n",
    "    observations, rewards, done, info = env.step(actions)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment viewer\n",
    "env.viewer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
