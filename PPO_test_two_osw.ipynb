{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicio del Episodio 1\n",
      "\n",
      "Resumen del Episodio 1\n",
      "Recompensas acumuladas: {'prey_0': 49.244069622005014}\n",
      "\n",
      "Inicio del Episodio 2\n",
      "\n",
      "Resumen del Episodio 2\n",
      "Recompensas acumuladas: {'prey_0': 44.00076519537924}\n",
      "\n",
      "Inicio del Episodio 3\n",
      "\n",
      "Resumen del Episodio 3\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 4\n",
      "\n",
      "Resumen del Episodio 4\n",
      "Recompensas acumuladas: {'prey_0': 36.99873162377422}\n",
      "\n",
      "Inicio del Episodio 5\n",
      "\n",
      "Resumen del Episodio 5\n",
      "Recompensas acumuladas: {'prey_0': 43.341301278324}\n",
      "\n",
      "Inicio del Episodio 6\n",
      "\n",
      "Resumen del Episodio 6\n",
      "Recompensas acumuladas: {'prey_0': 49.43802417048589}\n",
      "\n",
      "Inicio del Episodio 7\n",
      "\n",
      "Resumen del Episodio 7\n",
      "Recompensas acumuladas: {'prey_0': 41.21247185031807}\n",
      "\n",
      "Inicio del Episodio 8\n",
      "\n",
      "Resumen del Episodio 8\n",
      "Recompensas acumuladas: {'prey_0': 8.044957538509262}\n",
      "\n",
      "Inicio del Episodio 9\n",
      "\n",
      "Resumen del Episodio 9\n",
      "Recompensas acumuladas: {'prey_0': 40.71632382073294}\n",
      "\n",
      "Inicio del Episodio 10\n",
      "\n",
      "Resumen del Episodio 10\n",
      "Recompensas acumuladas: {'prey_0': 47.200976161746986}\n",
      "\n",
      "Inicio del Episodio 11\n",
      "\n",
      "Resumen del Episodio 11\n",
      "Recompensas acumuladas: {'prey_0': 37.53544423141279}\n",
      "\n",
      "Inicio del Episodio 12\n",
      "\n",
      "Resumen del Episodio 12\n",
      "Recompensas acumuladas: {'prey_0': 49.385895552197276}\n",
      "\n",
      "Inicio del Episodio 13\n",
      "\n",
      "Resumen del Episodio 13\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 14\n",
      "\n",
      "Resumen del Episodio 14\n",
      "Recompensas acumuladas: {'prey_0': 45.51055372026664}\n",
      "\n",
      "Inicio del Episodio 15\n",
      "\n",
      "Resumen del Episodio 15\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 16\n",
      "\n",
      "Resumen del Episodio 16\n",
      "Recompensas acumuladas: {'prey_0': 48.608631684032815}\n",
      "\n",
      "Inicio del Episodio 17\n",
      "\n",
      "Resumen del Episodio 17\n",
      "Recompensas acumuladas: {'prey_0': 17.812527449282157}\n",
      "\n",
      "Inicio del Episodio 18\n",
      "\n",
      "Resumen del Episodio 18\n",
      "Recompensas acumuladas: {'prey_0': 42.22841179397872}\n",
      "\n",
      "Inicio del Episodio 19\n",
      "\n",
      "Resumen del Episodio 19\n",
      "Recompensas acumuladas: {'prey_0': 43.12358297300623}\n",
      "\n",
      "Inicio del Episodio 20\n",
      "\n",
      "Resumen del Episodio 20\n",
      "Recompensas acumuladas: {'prey_0': 41.491682468045596}\n",
      "\n",
      "Inicio del Episodio 21\n",
      "\n",
      "Resumen del Episodio 21\n",
      "Recompensas acumuladas: {'prey_0': 39.17508229302548}\n",
      "\n",
      "Inicio del Episodio 22\n",
      "\n",
      "Resumen del Episodio 22\n",
      "Recompensas acumuladas: {'prey_0': 128.8754810606463}\n",
      "\n",
      "Inicio del Episodio 23\n",
      "\n",
      "Resumen del Episodio 23\n",
      "Recompensas acumuladas: {'prey_0': 38.646024819778845}\n",
      "\n",
      "Inicio del Episodio 24\n",
      "\n",
      "Resumen del Episodio 24\n",
      "Recompensas acumuladas: {'prey_0': 48.947637005241916}\n",
      "\n",
      "Inicio del Episodio 25\n",
      "\n",
      "Resumen del Episodio 25\n",
      "Recompensas acumuladas: {'prey_0': 5.079853943937685}\n",
      "\n",
      "Inicio del Episodio 26\n",
      "\n",
      "Resumen del Episodio 26\n",
      "Recompensas acumuladas: {'prey_0': 43.669820620196404}\n",
      "\n",
      "Inicio del Episodio 27\n",
      "\n",
      "Resumen del Episodio 27\n",
      "Recompensas acumuladas: {'prey_0': 27.862473713073918}\n",
      "\n",
      "Inicio del Episodio 28\n",
      "\n",
      "Resumen del Episodio 28\n",
      "Recompensas acumuladas: {'prey_0': -2.245696890224588}\n",
      "\n",
      "Inicio del Episodio 29\n",
      "\n",
      "Resumen del Episodio 29\n",
      "Recompensas acumuladas: {'prey_0': 38.66177322715673}\n",
      "\n",
      "Inicio del Episodio 30\n",
      "\n",
      "Resumen del Episodio 30\n",
      "Recompensas acumuladas: {'prey_0': 26.166239953589287}\n",
      "\n",
      "Inicio del Episodio 31\n",
      "\n",
      "Resumen del Episodio 31\n",
      "Recompensas acumuladas: {'prey_0': 47.45210683262606}\n",
      "\n",
      "Inicio del Episodio 32\n",
      "\n",
      "Resumen del Episodio 32\n",
      "Recompensas acumuladas: {'prey_0': 67.57441142420933}\n",
      "\n",
      "Inicio del Episodio 33\n",
      "\n",
      "Resumen del Episodio 33\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 34\n",
      "\n",
      "Resumen del Episodio 34\n",
      "Recompensas acumuladas: {'prey_0': 46.65496172038616}\n",
      "\n",
      "Inicio del Episodio 35\n",
      "\n",
      "Resumen del Episodio 35\n",
      "Recompensas acumuladas: {'prey_0': 46.54360973050878}\n",
      "\n",
      "Inicio del Episodio 36\n",
      "\n",
      "Resumen del Episodio 36\n",
      "Recompensas acumuladas: {'prey_0': 29.646640419146483}\n",
      "\n",
      "Inicio del Episodio 37\n",
      "\n",
      "Resumen del Episodio 37\n",
      "Recompensas acumuladas: {'prey_0': 49.49437674295303}\n",
      "\n",
      "Inicio del Episodio 38\n",
      "\n",
      "Resumen del Episodio 38\n",
      "Recompensas acumuladas: {'prey_0': 48.9776087997038}\n",
      "\n",
      "Inicio del Episodio 39\n",
      "\n",
      "Resumen del Episodio 39\n",
      "Recompensas acumuladas: {'prey_0': -6.619637087929279}\n",
      "\n",
      "Inicio del Episodio 40\n",
      "\n",
      "Resumen del Episodio 40\n",
      "Recompensas acumuladas: {'prey_0': 48.20919460149952}\n",
      "\n",
      "Inicio del Episodio 41\n",
      "\n",
      "Resumen del Episodio 41\n",
      "Recompensas acumuladas: {'prey_0': 48.96097982418734}\n",
      "\n",
      "Inicio del Episodio 42\n",
      "\n",
      "Resumen del Episodio 42\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 43\n",
      "\n",
      "Resumen del Episodio 43\n",
      "Recompensas acumuladas: {'prey_0': 85.3379514928852}\n",
      "\n",
      "Inicio del Episodio 44\n",
      "\n",
      "Resumen del Episodio 44\n",
      "Recompensas acumuladas: {'prey_0': 48.45827050208998}\n",
      "\n",
      "Inicio del Episodio 45\n",
      "\n",
      "Resumen del Episodio 45\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 46\n",
      "\n",
      "Resumen del Episodio 46\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 47\n",
      "\n",
      "Resumen del Episodio 47\n",
      "Recompensas acumuladas: {'prey_0': 48.64147843171156}\n",
      "\n",
      "Inicio del Episodio 48\n",
      "\n",
      "Resumen del Episodio 48\n",
      "Recompensas acumuladas: {'prey_0': 38.18013352761493}\n",
      "\n",
      "Inicio del Episodio 49\n",
      "\n",
      "Resumen del Episodio 49\n",
      "Recompensas acumuladas: {'prey_0': 40.98656220626013}\n",
      "\n",
      "Inicio del Episodio 50\n",
      "\n",
      "Resumen del Episodio 50\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 51\n",
      "\n",
      "Resumen del Episodio 51\n",
      "Recompensas acumuladas: {'prey_0': 49.4526793431975}\n",
      "\n",
      "Inicio del Episodio 52\n",
      "\n",
      "Resumen del Episodio 52\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 53\n",
      "\n",
      "Resumen del Episodio 53\n",
      "Recompensas acumuladas: {'prey_0': 26.687219065670476}\n",
      "\n",
      "Inicio del Episodio 54\n",
      "\n",
      "Resumen del Episodio 54\n",
      "Recompensas acumuladas: {'prey_0': 25.992870849176963}\n",
      "\n",
      "Inicio del Episodio 55\n",
      "\n",
      "Resumen del Episodio 55\n",
      "Recompensas acumuladas: {'prey_0': 38.75837100075648}\n",
      "\n",
      "Inicio del Episodio 56\n",
      "\n",
      "Resumen del Episodio 56\n",
      "Recompensas acumuladas: {'prey_0': 57.46350343951105}\n",
      "\n",
      "Inicio del Episodio 57\n",
      "\n",
      "Resumen del Episodio 57\n",
      "Recompensas acumuladas: {'prey_0': 64.25238882659104}\n",
      "\n",
      "Inicio del Episodio 58\n",
      "\n",
      "Resumen del Episodio 58\n",
      "Recompensas acumuladas: {'prey_0': 43.51333542384369}\n",
      "\n",
      "Inicio del Episodio 59\n",
      "\n",
      "Resumen del Episodio 59\n",
      "Recompensas acumuladas: {'prey_0': 43.63333085427508}\n",
      "\n",
      "Inicio del Episodio 60\n",
      "\n",
      "Resumen del Episodio 60\n",
      "Recompensas acumuladas: {'prey_0': 48.8033060948344}\n",
      "\n",
      "Inicio del Episodio 61\n",
      "\n",
      "Resumen del Episodio 61\n",
      "Recompensas acumuladas: {'prey_0': 99.5}\n",
      "\n",
      "Inicio del Episodio 62\n",
      "\n",
      "Resumen del Episodio 62\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 63\n",
      "\n",
      "Resumen del Episodio 63\n",
      "Recompensas acumuladas: {'prey_0': 49.49972496169711}\n",
      "\n",
      "Inicio del Episodio 64\n",
      "\n",
      "Resumen del Episodio 64\n",
      "Recompensas acumuladas: {'prey_0': 12.350493189729896}\n",
      "\n",
      "Inicio del Episodio 65\n",
      "\n",
      "Resumen del Episodio 65\n",
      "Recompensas acumuladas: {'prey_0': 14.270053798410903}\n",
      "\n",
      "Inicio del Episodio 66\n",
      "\n",
      "Resumen del Episodio 66\n",
      "Recompensas acumuladas: {'prey_0': 15.431741381521265}\n",
      "\n",
      "Inicio del Episodio 67\n",
      "\n",
      "Resumen del Episodio 67\n",
      "Recompensas acumuladas: {'prey_0': 36.58129952129494}\n",
      "\n",
      "Inicio del Episodio 68\n",
      "\n",
      "Resumen del Episodio 68\n",
      "Recompensas acumuladas: {'prey_0': 18.729666503210765}\n",
      "\n",
      "Inicio del Episodio 69\n",
      "\n",
      "Resumen del Episodio 69\n",
      "Recompensas acumuladas: {'prey_0': 16.099955062094104}\n",
      "\n",
      "Inicio del Episodio 70\n",
      "\n",
      "Resumen del Episodio 70\n",
      "Recompensas acumuladas: {'prey_0': 49.486162710494696}\n",
      "\n",
      "Inicio del Episodio 71\n",
      "\n",
      "Resumen del Episodio 71\n",
      "Recompensas acumuladas: {'prey_0': 44.53932708211873}\n",
      "\n",
      "Inicio del Episodio 72\n",
      "\n",
      "Resumen del Episodio 72\n",
      "Recompensas acumuladas: {'prey_0': 43.59974006601283}\n",
      "\n",
      "Inicio del Episodio 73\n",
      "\n",
      "Resumen del Episodio 73\n",
      "Recompensas acumuladas: {'prey_0': 46.6451830444339}\n",
      "\n",
      "Inicio del Episodio 74\n",
      "\n",
      "Resumen del Episodio 74\n",
      "Recompensas acumuladas: {'prey_0': 12.63881200876014}\n",
      "\n",
      "Inicio del Episodio 75\n",
      "\n",
      "Resumen del Episodio 75\n",
      "Recompensas acumuladas: {'prey_0': 38.36410691021466}\n",
      "\n",
      "Inicio del Episodio 76\n",
      "\n",
      "Resumen del Episodio 76\n",
      "Recompensas acumuladas: {'prey_0': 97.77745836501133}\n",
      "\n",
      "Inicio del Episodio 77\n",
      "\n",
      "Resumen del Episodio 77\n",
      "Recompensas acumuladas: {'prey_0': 42.04003901153543}\n",
      "\n",
      "Inicio del Episodio 78\n",
      "\n",
      "Resumen del Episodio 78\n",
      "Recompensas acumuladas: {'prey_0': 47.14866624210915}\n",
      "\n",
      "Inicio del Episodio 79\n",
      "\n",
      "Resumen del Episodio 79\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 80\n",
      "\n",
      "Resumen del Episodio 80\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 81\n",
      "\n",
      "Resumen del Episodio 81\n",
      "Recompensas acumuladas: {'prey_0': 41.90322097375051}\n",
      "\n",
      "Inicio del Episodio 82\n",
      "\n",
      "Resumen del Episodio 82\n",
      "Recompensas acumuladas: {'prey_0': 44.91122795804301}\n",
      "\n",
      "Inicio del Episodio 83\n",
      "\n",
      "Resumen del Episodio 83\n",
      "Recompensas acumuladas: {'prey_0': 34.86978149279658}\n",
      "\n",
      "Inicio del Episodio 84\n",
      "\n",
      "Resumen del Episodio 84\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 85\n",
      "\n",
      "Resumen del Episodio 85\n",
      "Recompensas acumuladas: {'prey_0': 46.32320962317669}\n",
      "\n",
      "Inicio del Episodio 86\n",
      "\n",
      "Resumen del Episodio 86\n",
      "Recompensas acumuladas: {'prey_0': 49.5}\n",
      "\n",
      "Inicio del Episodio 87\n",
      "\n",
      "Resumen del Episodio 87\n",
      "Recompensas acumuladas: {'prey_0': 41.46413242851137}\n",
      "\n",
      "Inicio del Episodio 88\n",
      "\n",
      "Resumen del Episodio 88\n",
      "Recompensas acumuladas: {'prey_0': 48.51130873925575}\n",
      "\n",
      "Inicio del Episodio 89\n",
      "\n",
      "Resumen del Episodio 89\n",
      "Recompensas acumuladas: {'prey_0': 49.35668042889223}\n",
      "\n",
      "Inicio del Episodio 90\n",
      "\n",
      "Resumen del Episodio 90\n",
      "Recompensas acumuladas: {'prey_0': 22.768214960307166}\n",
      "\n",
      "Inicio del Episodio 91\n",
      "\n",
      "Resumen del Episodio 91\n",
      "Recompensas acumuladas: {'prey_0': 39.62400299277734}\n",
      "\n",
      "Inicio del Episodio 92\n",
      "\n",
      "Resumen del Episodio 92\n",
      "Recompensas acumuladas: {'prey_0': 46.68174355414971}\n",
      "\n",
      "Inicio del Episodio 93\n",
      "\n",
      "Resumen del Episodio 93\n",
      "Recompensas acumuladas: {'prey_0': 89.5}\n",
      "\n",
      "Inicio del Episodio 94\n",
      "\n",
      "Resumen del Episodio 94\n",
      "Recompensas acumuladas: {'prey_0': 49.23801521335709}\n",
      "\n",
      "Inicio del Episodio 95\n",
      "\n",
      "Resumen del Episodio 95\n",
      "Recompensas acumuladas: {'prey_0': 28.644366791140175}\n",
      "\n",
      "Inicio del Episodio 96\n",
      "\n",
      "Resumen del Episodio 96\n",
      "Recompensas acumuladas: {'prey_0': 23.13308345387075}\n",
      "\n",
      "Inicio del Episodio 97\n",
      "\n",
      "Resumen del Episodio 97\n",
      "Recompensas acumuladas: {'prey_0': 47.54313525565189}\n",
      "\n",
      "Inicio del Episodio 98\n",
      "\n",
      "Resumen del Episodio 98\n",
      "Recompensas acumuladas: {'prey_0': 46.977529769495646}\n",
      "\n",
      "Inicio del Episodio 99\n",
      "\n",
      "Resumen del Episodio 99\n",
      "Recompensas acumuladas: {'prey_0': 49.19962809346122}\n",
      "\n",
      "Inicio del Episodio 100\n",
      "\n",
      "Resumen del Episodio 100\n",
      "Recompensas acumuladas: {'prey_0': 45.35201446593043}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from NaturalEnv import natural_env_v0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inicializar el entorno\n",
    "#env = natural_env_v0.parallel_env(render_mode=None, max_cycles=100)\n",
    "env = natural_env_v0.parallel_env(render_mode=None, max_cycles=100, num_predators=0, num_prey=1,\n",
    "                                  num_obstacles = 0, num_food=1, num_water=1, num_forests=0)\n",
    "\n",
    "observations, infos = env.reset(seed=42)\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "clip_epsilon = 0.2\n",
    "action_space = 5\n",
    "epsilon = 0.9 # Tasa de exploración elevada\n",
    "\n",
    "# Definir la red neuronal para la política\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)  # Dropout con probabilidad de 0.3\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)  # Dropout con probabilidad de 0.3\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Aplicar Dropout después de la primera capa\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Aplicar Dropout después de la segunda capa\n",
    "        logits = self.fc3(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "# Crear redes para las presas y los depredadores\n",
    "input_dim = observations['prey_0'].shape[0]\n",
    "policy_prey = PolicyNetwork(input_dim, action_space)\n",
    "#policy_predator = PolicyNetwork(input_dim, action_space)\n",
    "\n",
    "optimizer_prey = optim.Adam(policy_prey.parameters(), lr=learning_rate)\n",
    "#optimizer_predator = optim.Adam(policy_predator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Funciones de ayuda\n",
    "def get_action(observation, policy_net, exploration=True, epsilon=0.9):\n",
    "    observation = torch.tensor(observation, dtype=torch.float32)\n",
    "    action_probs = policy_net(observation)\n",
    "    action_dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        action = random.randint(0, action_space - 1)\n",
    "        log_prob = torch.log(torch.tensor(1.0 / action_space, dtype=torch.float32))\n",
    "        return action, log_prob\n",
    "\n",
    "    action = action_dist.sample()\n",
    "    return action.item(), action_dist.log_prob(action)\n",
    "\n",
    "\n",
    "def ppo_update(experience, policy_net, optimizer, clip_eps=clip_epsilon):\n",
    "    \"\"\"Actualiza el modelo utilizando PPO.\"\"\"\n",
    "    log_probs_old = []\n",
    "    rewards = []\n",
    "    log_probs_new = []\n",
    "\n",
    "    # Recolectar datos de experiencia\n",
    "    for state, action, reward, log_prob_old in experience:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        log_prob_old = log_prob_old.clone().detach()  # Corregido\n",
    "\n",
    "        # Obtener la probabilidad logarítmica de la acción con la política actual\n",
    "        action_probs = policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        log_prob_new = action_dist.log_prob(torch.tensor(action))\n",
    "\n",
    "        log_probs_old.append(log_prob_old)\n",
    "        rewards.append(reward)\n",
    "        log_probs_new.append(log_prob_new)\n",
    "\n",
    "    log_probs_old = torch.stack(log_probs_old)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    log_probs_new = torch.stack(log_probs_new)\n",
    "\n",
    "    # Calcular ventaja\n",
    "    advantage = rewards - rewards.mean()\n",
    "\n",
    "    # Calcular el ratio y aplicar clipping\n",
    "    ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "    clipped_ratios = torch.clamp(ratios, 1 - clip_eps, 1 + clip_eps)\n",
    "    loss = -torch.min(ratios * advantage, clipped_ratios * advantage).mean()\n",
    "\n",
    "    # Actualizar los parámetros de la política\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    observations, infos = env.reset()\n",
    "    experience_prey = []\n",
    "    #experience_predator = []\n",
    "    total_rewards = {agent: 0 for agent in env.agents}\n",
    "    \n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while env.agents:\n",
    "        actions = {}\n",
    "        \n",
    "\n",
    "        # Seleccionar acciones para cada agente\n",
    "        for agent in env.agents:\n",
    "            obs = observations[agent]\n",
    "            obs = (obs - np.mean(obs)) / (np.std(obs) + 1e-5)\n",
    "            if \"prey\" in agent:\n",
    "                action, log_prob = get_action(obs, policy_prey, epsilon=epsilon)\n",
    "                actions[agent] = action\n",
    "            #elif \"predator\" in agent:\n",
    "             #   action, log_prob = get_action(obs, policy_predator)\n",
    "              #  actions[agent] = action\n",
    "\n",
    "        # Tomar un paso en el entorno\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Recolectar experiencia después de obtener la recompensa\n",
    "        for agent in env.agents:\n",
    "            obs = observations[agent]\n",
    "            if \"prey\" in agent:\n",
    "                action = actions[agent]\n",
    "                log_prob = get_action(obs, policy_prey)[1]\n",
    "                experience_prey.append((obs, action, rewards[agent], log_prob))\n",
    "            #elif \"predator\" in agent:\n",
    "             #   action = actions[agent]\n",
    "              #  log_prob = get_action(obs, policy_predator)[1]\n",
    "               # experience_predator.append((obs, action, rewards[agent], log_prob))\n",
    "\n",
    "            total_rewards[agent] += rewards[agent]\n",
    "\n",
    "        # Remover agentes que terminaron\n",
    "        env.agents = [agent for agent in env.agents if not (terminations[agent] or truncations[agent])]\n",
    "\n",
    "    # Actualizar la política utilizando PPO\n",
    "    ppo_update(experience_prey, policy_prey, optimizer_prey)\n",
    "    #ppo_update(experience_predator, policy_predator, optimizer_predator)\n",
    "\n",
    "    # Reducir gradualmente la exploración\n",
    "    epsilon = max(0.1, epsilon * 0.99)\n",
    "\n",
    "    print(f\"\\nResumen del Episodio {episode + 1}\")\n",
    "    print(f\"Recompensas acumuladas: {total_rewards}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
