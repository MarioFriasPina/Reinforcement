{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Neural Network with pytorch for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model that recieves the observation and returns an action\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(8, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    ")\n",
    "\n",
    "# Create a model for reinforcement learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_1304\\2252747985.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load('_policy_net.pth'))\n",
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_1304\\2252747985.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_net.load_state_dict(torch.load('_target_net.pth'))\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 500\n",
    "batch_size = 128\n",
    "\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.05  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Exploration decay rate\n",
    "\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "tau = 0.0005\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "policy_net = DQN(layers).to(device)\n",
    "target_net = DQN(layers).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Initialize the policy network and target network if it exists in the disk\n",
    "if os.path.exists('_policy_net.pth'):\n",
    "    policy_net.load_state_dict(torch.load('_policy_net.pth'))\n",
    "if os.path.exists('_target_net.pth'):\n",
    "    target_net.load_state_dict(torch.load('_target_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(observation, action_space, epsilon, explotation=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    \n",
    "    steps_done += 1\n",
    "\n",
    "    if explotation:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(observation).max(1)[1].view(1, 1)\n",
    "    elif sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(observation).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def learn():\n",
    "    \"\"\"\n",
    "    Function that performs a learning step using DQN\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)    \n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 200)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test environment\n",
    "\n",
    "#### Lunar Lander Environment\n",
    "\n",
    "- Observation Space: 8\n",
    "    1. X position of the lander\n",
    "    2. Y position of the lander\n",
    "    3. X velocity of the lander\n",
    "    4. Y velocity of the lander\n",
    "    5. Angle of the lander\n",
    "    6. Angular velocity of the lander\n",
    "    7. Left Leg Contact (0 or 1)\n",
    "    8. Right Leg Contact (0 or 1)\n",
    "\n",
    "- Action Space: 4\n",
    "    0. Do nothing\n",
    "    1. Fire left Engine\n",
    "    2. Fire main Engine\n",
    "    3. Fire right Engine\n",
    "\n",
    "- Rewards:\n",
    "    - -100 for Collision\n",
    "    - +100 for Landing\n",
    "\n",
    "    - is increased/decreased the closer/further the lander is to the landing pad.\n",
    "    - is increased/decreased the slower/faster the lander is moving.\n",
    "    - is decreased the more the lander is tilted (angle not horizontal).\n",
    "    - is increased by 10 points for each leg that is in contact with the ground.\n",
    "    - is decreased by 0.03 points each frame a side engine is firing.\n",
    "    - is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m     memory\u001b[38;5;241m.\u001b[39mpush(frame, action, reward, next_frame, done)\n\u001b[0;32m     51\u001b[0m     frame \u001b[38;5;241m=\u001b[39m next_frame\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon_min, epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Update the target network        \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mlearn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m batch \u001b[38;5;241m=\u001b[39m Transition(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions))\n\u001b[0;32m     23\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, batch\u001b[38;5;241m.\u001b[39mnext_state)), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m---> 24\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m     27\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_history = []\n",
    "plt.ion()\n",
    "\n",
    "def update_plot(episode, max_episodes, show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(reward_history, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Final Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "    frame = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    episode_over = False\n",
    "\n",
    "    for t in count():\n",
    "        action = choose_action(frame, env.action_space, epsilon)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            next_frame = None\n",
    "            episode_over = True\n",
    "        else:\n",
    "            next_frame = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = torch.tensor([episode_over], device=device)\n",
    "\n",
    "            memory.push(frame, action, reward, next_frame, done)\n",
    "\n",
    "            frame = next_frame\n",
    "            learn()\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Update the target network        \n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * (tau) + target_net_state_dict[key] * (1 - tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "    \n",
    "        if episode_over:\n",
    "            reward_history.append(total_reward)\n",
    "            break\n",
    "\n",
    "    update_plot(episode + 1, max_episodes)\n",
    "\n",
    "update_plot(episode + 1, max_episodes, show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights to disk\n",
    "torch.save(policy_net.state_dict(), '_policy_net.pth')\n",
    "torch.save(target_net.state_dict(), '_target_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_human = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "for episode in range(10):\n",
    "    state, info = env_human.reset()\n",
    "    frame = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = choose_action(frame, env.action_space, epsilon, explotation=True)\n",
    "        observation, reward, terminated, truncated, info = env_human.step(action.item())\n",
    "\n",
    "        frame = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        if terminated or truncated:\n",
    "            episode_over = True\n",
    "\n",
    "env_human.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
