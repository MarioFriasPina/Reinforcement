{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicio del Episodio 1\n",
      "Recompensas acumuladas: {'prey_0': -24.6245222857422, 'prey_1': 8.399999999999995, 'prey_2': -46.02781424473212, 'prey_3': -5.587157694768834, 'prey_4': 2.400000000000001, 'prey_5': 11.399999999999999, 'predator_0': -15.94555310376971, 'predator_1': -14.872268014593276}\n",
      "\n",
      "Inicio del Episodio 2\n",
      "Recompensas acumuladas: {'prey_0': 2.400000000000001, 'prey_1': 2.400000000000001, 'prey_2': 2.400000000000001, 'prey_3': 7.399999999999999, 'prey_4': -36.86832441538518, 'prey_5': -2.3958489991743046, 'predator_0': -20.91062022173938, 'predator_1': -10.20672414424847}\n",
      "\n",
      "Inicio del Episodio 3\n",
      "Recompensas acumuladas: {'prey_0': -27.65334663777228, 'prey_1': 2.400000000000001, 'prey_2': 2.400000000000001, 'prey_3': -35.31269661231628, 'prey_4': 13.399999999999995, 'prey_5': -13.377532925479219, 'predator_0': -23.811878842393764, 'predator_1': -15.783645919318353}\n",
      "\n",
      "Inicio del Episodio 4\n",
      "Recompensas acumuladas: {'prey_0': 2.400000000000001, 'prey_1': 2.400000000000001, 'prey_2': 2.400000000000001, 'prey_3': -28.266947877240682, 'prey_4': 2.400000000000001, 'prey_5': -45.396025677369096, 'predator_0': -33.773392209561415, 'predator_1': -9.904955847449173}\n",
      "\n",
      "Inicio del Episodio 5\n",
      "Recompensas acumuladas: {'prey_0': -20.49141283487897, 'prey_1': -51.94353737062866, 'prey_2': 2.400000000000001, 'prey_3': 11.399999999999993, 'prey_4': 1.8193869347768616, 'prey_5': -73.86277670963555, 'predator_0': -22.56222422827482, 'predator_1': -54.84525769891382}\n",
      "\n",
      "Inicio del Episodio 6\n",
      "Recompensas acumuladas: {'prey_0': 2.400000000000001, 'prey_1': -57.70791653418683, 'prey_2': 2.400000000000001, 'prey_3': 5.399999999999995, 'prey_4': -20.65242921827724, 'prey_5': 2.400000000000001, 'predator_0': -9.06892697118659, 'predator_1': -38.97687093718116}\n",
      "\n",
      "Inicio del Episodio 7\n",
      "Recompensas acumuladas: {'prey_0': -89.58159598707907, 'prey_1': -25.323518162063337, 'prey_2': 2.400000000000001, 'prey_3': -53.570198289455405, 'prey_4': 2.400000000000001, 'prey_5': -49.37584859466811, 'predator_0': -12.07102069802927, 'predator_1': -12.330283284480977}\n",
      "\n",
      "Inicio del Episodio 8\n",
      "Recompensas acumuladas: {'prey_0': -166.80789433646493, 'prey_1': 2.400000000000001, 'prey_2': -38.58376351593792, 'prey_3': 2.400000000000001, 'prey_4': 2.400000000000001, 'prey_5': -55.14878982008516, 'predator_0': -10.584644660124546, 'predator_1': -27.901369783722398}\n",
      "\n",
      "Inicio del Episodio 9\n",
      "Recompensas acumuladas: {'prey_0': 2.400000000000001, 'prey_1': -68.90267679552201, 'prey_2': -2.4438045788678644, 'prey_3': -50.677975665070996, 'prey_4': 5.4, 'prey_5': 3.400000000000001, 'predator_0': -11.074426359259391, 'predator_1': -10.628005703249933}\n",
      "\n",
      "Inicio del Episodio 10\n",
      "Recompensas acumuladas: {'prey_0': -20.575885325320577, 'prey_1': 2.400000000000001, 'prey_2': -8.444560541453141, 'prey_3': 2.400000000000001, 'prey_4': -41.918198888425394, 'prey_5': 4.4, 'predator_0': -11.043368944201594, 'predator_1': -13.495958194826095}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "# Inicializar el entorno\n",
    "env = natural_env_v0.parallel_env(render_mode=None)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "action_space = 5\n",
    "epsilon = 0.5\n",
    "\n",
    "# Inicializar modelos de Q-learning para cada agente\n",
    "model_params = {agent: np.zeros((observations[agent].shape[0], action_space)) for agent in env.agents}\n",
    "\n",
    "# Funciones de ayuda\n",
    "def calc_state(observation):\n",
    "    return int(observation.sum()) % observation.shape[0]\n",
    "\n",
    "def get_action(state, model_params, exploration=True):\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    else:\n",
    "        return model_params[state].argmax()\n",
    "\n",
    "def train(experience, model_params, lr=learning_rate, df=discount_factor):\n",
    "    \"\"\"Entrena el modelo actualizando parámetros usando Q-learning.\"\"\"\n",
    "    for prev_state, action_taken, state, reward in reversed(experience):\n",
    "        best_next_action = model_params[state].max()\n",
    "        target = reward + df * best_next_action\n",
    "        model_params[prev_state][action_taken] += lr * (target - model_params[prev_state][action_taken])\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "    observations, infos = env.reset()\n",
    "    experience = {agent: [] for agent in env.agents}\n",
    "    total_rewards = {agent: 0 for agent in env.agents}\n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while env.agents:\n",
    "        # Seleccionar acciones para cada agente\n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            state = calc_state(observations[agent])\n",
    "            action = get_action(state, model_params[agent])\n",
    "            actions[agent] = action\n",
    "\n",
    "        # Tomar un paso en el entorno\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Registrar experiencia y actualizar recompensas acumuladas\n",
    "        for agent in env.agents:\n",
    "            state = calc_state(observations[agent])\n",
    "            prev_state = calc_state(observations[agent])\n",
    "            action = actions[agent]\n",
    "            reward = rewards[agent]\n",
    "            total_rewards[agent] += reward\n",
    "\n",
    "            experience[agent].append([prev_state, action, state, reward])\n",
    "\n",
    "            # Verificar si el agente ha terminado\n",
    "            if terminations[agent] or truncations[agent]:\n",
    "                env.agents.remove(agent)\n",
    "\n",
    "    # Entrenar al final del episodio\n",
    "    for agent in experience:\n",
    "        train(experience[agent], model_params[agent])\n",
    "\n",
    "    # Mostrar las recompensas acumuladas\n",
    "    print(f\"Recompensas acumuladas: {total_rewards}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
