{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Centralized Training, Decentralized Execution (CTDE): Each agent acts independently during execution but trains with access to global information (e.g., observations and actions of all agents).\n",
    "\n",
    "2. Network Architecture:\n",
    "    - Actor Network: Determines the policy (actions) for each agent.\n",
    "    - Critic Network: Evaluates the Q-value for each agent's action, conditioned on all agents' observations and actions.\n",
    "3. Replay Buffer: A shared memory buffer to store transitions (state, actions, rewards, next_state) for all agents.\n",
    "\n",
    "4. Soft Target Updates: Slowly update target networks for stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 16, 'adversary_1': 16, 'adversary_2': 16, 'agent_0': 14}\n",
      "{'adversary_0': 5, 'adversary_1': 5, 'adversary_2': 5, 'agent_0': 5}\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_tag_v3\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize parallel environment\n",
    "env = simple_tag_v3.parallel_env(max_cycles=128, continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "# Get observation and action spaces\n",
    "obs_spaces = {agent: env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_spaces = {agent: env.action_space(agent).shape[0] for agent in env.agents}\n",
    "agents = env.agents  # List of agents\n",
    "\n",
    "\n",
    "print(obs_spaces)\n",
    "print(action_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        input_dim = obs_dim + action_dim  # Critic gets all obs & actions\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs, actions):\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ParallelReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, observations, actions, rewards, next_observations, dones):\n",
    "        # Store a single transition for all agents\n",
    "        self.buffer.append((observations, actions, rewards, next_observations, dones))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert dictionary data into tensors\n",
    "        obs = {agent: torch.tensor(np.array([b[agent] for b in obs]), dtype=torch.float32, device=device) for agent in obs[0]}\n",
    "        actions = {agent: torch.tensor(np.array([b[agent] for b in actions]), dtype=torch.float32, device=device) for agent in actions[0]}\n",
    "        rewards = {agent: torch.tensor([b[agent] for b in rewards], dtype=torch.float32, device=device) for agent in rewards[0]}\n",
    "        next_obs = {agent: torch.tensor(np.array([b[agent] for b in next_obs]), dtype=torch.float32, device=device) for agent in next_obs[0]}\n",
    "        dones = {agent: torch.tensor([b[agent] for b in dones], dtype=torch.float32, device=device) for agent in dones[0]}\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(actors, critics, target_actors, target_critics, buffer, batch_size, gamma=0.95, tau=0.01):\n",
    "    obs, actions, rewards, next_obs, dones = buffer.sample(batch_size)\n",
    "    \n",
    "    # Centralized Q-value update for each agent\n",
    "    for i, agent in enumerate(agents):\n",
    "        # Get target actions for all agents\n",
    "        target_actions = torch.cat([target_actors[j](next_obs[other]) for j, other in enumerate(agents)], dim=-1)\n",
    "        obs_concat = torch.cat([obs[other] for other in agents], dim=-1)\n",
    "        next_obs_concat = torch.cat([next_obs[other] for other in agents], dim=-1)\n",
    "        \n",
    "        # Compute target Q-value\n",
    "        target_q = target_critics[i](next_obs_concat, target_actions).detach()\n",
    "        y = rewards[agent] + gamma * (1 - dones[agent]) * target_q.squeeze()\n",
    "        \n",
    "        # Predicted Q-value\n",
    "        actions_concat = torch.cat([actions[other] for other in agents], dim=-1)\n",
    "        current_q = critics[i](obs_concat, actions_concat).squeeze()\n",
    "        \n",
    "        # Critic Loss\n",
    "        critic_loss = torch.nn.functional.mse_loss(current_q, y)\n",
    "        critics[i].optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critics[i].optimizer.step()\n",
    "\n",
    "    # Policy (actor) update\n",
    "    for i, agent in enumerate(agents):\n",
    "        current_actions = torch.cat(\n",
    "            [actors[j](obs[other]) if other == agent else actions[other].detach() for j, other in enumerate(agents)], dim=-1\n",
    "        )\n",
    "        actor_loss = -critics[i](obs_concat, current_actions).mean()\n",
    "        actors[i].optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actors[i].optimizer.step()\n",
    "\n",
    "    # Soft update for target networks\n",
    "    for i, agent in enumerate(agents):\n",
    "        for target_param, param in zip(target_critics[i].parameters(), critics[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(target_actors[i].parameters(), actors[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 complete.\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Choose actions for each agent\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43magent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mactors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     next_obs, rewards, terminated, truncated,  _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Choose actions for each agent\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent: actors[i](\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agents)}\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     next_obs, rewards, terminated, truncated,  _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize actors, critics, target networks, and optimizers\n",
    "actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "target_actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "target_critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "\n",
    "# Optimizers\n",
    "for actor, critic in zip(actors, critics):\n",
    "    actor.optimizer = Adam(actor.parameters(), lr=1e-3)\n",
    "    critic.optimizer = Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = ParallelReplayBuffer()\n",
    "\n",
    "# Early Stopping Parameters\n",
    "best_reward = {agent: -np.inf for agent in agents}\n",
    "patience = 10  # Number of episodes to wait for improvement\n",
    "patience_counter = 0  # Counter to track episodes since last improvement\n",
    "min_delta = 1e-4  # Minimum change in reward to be considered an improvement\n",
    "\n",
    "# Main training loop\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Get initial observations\n",
    "    done = defaultdict(bool, {agent: False for agent in agents})\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "    \n",
    "    while not all(done.values()):\n",
    "        # Choose actions for each agent\n",
    "        actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, rewards, terminated, truncated,  _ = env.step(actions)\n",
    "\n",
    "        # Compute reward for each agent\n",
    "        for agent in agents:\n",
    "            episode_reward[agent] += rewards[agent]\n",
    "\n",
    "        # Update done flag\n",
    "        done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "                \n",
    "        # Store transition in replay buffer\n",
    "        buffer.add(obs, actions, rewards, next_obs, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        # Training step if enough data in buffer\n",
    "        if buffer.size() > batch_size:\n",
    "            train_step(actors, critics, target_actors, target_critics, buffer, batch_size)\n",
    "\n",
    "    # Check for early stopping, if no agent has improved by min_delta in the last patience episodes\n",
    "    got_better = False\n",
    "    for agent in agents:\n",
    "        if episode_reward[agent] > best_reward[agent] + min_delta:\n",
    "            best_reward[agent] = episode_reward[agent]\n",
    "            got_better = True\n",
    "            \n",
    "\n",
    "    if got_better:\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f\"Episode {episode} complete.\", end=\"\\r\", flush=True)\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "human_env = simple_tag_v3.parallel_env(render_mode=\"human\", continuous_actions=True)\n",
    "\n",
    "obs, _ = human_env.reset()\n",
    "done = defaultdict(bool, {agent: False for agent in agents})\n",
    "\n",
    "while not all(done.values()):\n",
    "    actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "    obs, _, terminated, truncated, _ = human_env.step(actions)\n",
    "    done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "\n",
    "human_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
