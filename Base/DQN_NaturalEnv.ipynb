{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import os\n",
    "\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prey_0\n",
      "(16,)\n",
      "[ 0.0000000e+00  0.0000000e+00  4.3388960e-01 -4.5924118e-01\n",
      "  5.0000000e+01  5.0000000e+01  0.0000000e+00  0.0000000e+00\n",
      " -8.4448498e-01 -1.9646665e-01 -3.4622416e-01  4.0890381e-02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "args = {'render_mode': None, 'max_cycles': 256, 'num_predators': 0, 'num_prey': 1, 'num_obstacles': 0, 'num_food': 1, 'num_water': 1, 'num_forests': 0}\n",
    "\n",
    "env = natural_env_v0.parallel_env(**args)\n",
    "\n",
    "observations, infos = env.reset()\n",
    "\n",
    "for agent_name in env.agents:\n",
    "    print(agent_name)\n",
    "    nn_input_size = observations[agent_name].shape\n",
    "    print(nn_input_size)\n",
    "    print(observations[agent_name])\n",
    "\n",
    "num_actions = env.action_space(agent_name).n\n",
    "\n",
    "# Basic model that recieves the observation and returns an action\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(nn_input_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, num_actions),\n",
    ")\n",
    "\n",
    "\n",
    "# Changes the batches into the correct format for the DQN\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Create a model for reinforcement learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Keeps the memory of previous steps, and gives ways to access it quickly\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Number of transitions sampled from replay buffer\n",
    "\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.05  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Exploration decay rate\n",
    "\n",
    "lr = 0.01  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "tau = 0.0005  # Target network update rate\n",
    "\n",
    "policy_net = DQN(layers).to(device)\n",
    "target_net = DQN(layers).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Keeps record of previous steps in a batch\n",
    "memory = {agent_name: ReplayMemory(10000) for agent_name in env.agents}\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(observation, action_space, epsilon, explotation=False):\n",
    "    sample = random.random()\n",
    "\n",
    "    if explotation:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(observation).max(1)[1].view(1, 1)\n",
    "    elif sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(observation).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def learn(agent):\n",
    "    \"\"\"\n",
    "    Function that performs a learning step using DQN\n",
    "    \"\"\"\n",
    "    if len(memory[agent]) < batch_size:\n",
    "        return\n",
    "\n",
    "    transitions = memory[agent].sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 200)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m         memory[agent_name]\u001b[38;5;241m.\u001b[39mpush(frame[agent_name], actions_tensors[agent_name], reward, next_frame, episode_over)\n\u001b[1;32m     85\u001b[0m         frame[agent_name] \u001b[38;5;241m=\u001b[39m next_frame\n\u001b[0;32m---> 86\u001b[0m         \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon_min, epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Update the target network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     34\u001b[0m     next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m target_net(non_final_next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     36\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m gamma) \u001b[38;5;241m+\u001b[39m reward_batch\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth_l1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_action_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_state_action_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/torch/nn/functional.py:3676\u001b[0m, in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction, beta)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(\n\u001b[1;32m   3673\u001b[0m         expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3674\u001b[0m     )\n\u001b[1;32m   3675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth_l1_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_history = {agent_name: [] for agent_name in env.agents}\n",
    "agent_names = list(env.agents)\n",
    "\n",
    "max_rewards = {agent_name: 0 for agent_name in env.agents}\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def update_plot_multi_agent(episode, max_episodes, show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(f'Final Result:')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "\n",
    "    for agent_name in agent_names:\n",
    "        agent_name = agent_names[0]\n",
    "        rewards_t = torch.tensor(reward_history[agent_name], dtype=torch.float)\n",
    "        plt.plot(rewards_t.numpy(), label=agent_name)\n",
    "\n",
    "        # Plot moving average of last 10 rewards\n",
    "        if len(rewards_t) >= 10:\n",
    "            means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            plt.plot(means.numpy())            \n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "max_episodes = 500\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    frame = {}\n",
    "    # Get the initial frame for each agent\n",
    "    for agent_name in env.agents:\n",
    "        frame[agent_name] = torch.tensor(state[agent_name], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Get the total reward for each agent\n",
    "    total_reward = {agent_name: 0 for agent_name in env.agents}\n",
    "    episode_over = False\n",
    "\n",
    "    # Keep going\n",
    "    while True:\n",
    "        actions = {}\n",
    "        actions_tensors = {}\n",
    "        # Choose actions for each agent\n",
    "        for agent_name in env.agents:\n",
    "            action = choose_action(frame[agent_name], env.action_space(agent_name), epsilon)\n",
    "            actions[agent_name] = action.item()\n",
    "            actions_tensors[agent_name] = action\n",
    "\n",
    "\n",
    "        # Do all actions at the same time\n",
    "        observation, reward, terminated, truncated, info = env.step(actions)\n",
    "\n",
    "        if all(terminated.values()) or all(truncated.values()):\n",
    "            episode_over = True\n",
    "            for agent_name in env.agents:\n",
    "                total_reward[agent_name] += reward[agent_name]\n",
    "        else:\n",
    "            # Get the results of the actions for each agent\n",
    "            for agent_name in env.agents:\n",
    "                \n",
    "                total_reward[agent_name] += reward[agent_name]\n",
    "                # Save the observation in a tensor\n",
    "                next_frame = torch.tensor(observation[agent_name], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                # Convert the reward in a tensor\n",
    "                reward = torch.tensor([reward[agent_name]], device=device)\n",
    "                #done = torch.tensor([episode_over], device=device)\n",
    "\n",
    "                memory[agent_name].push(frame[agent_name], actions_tensors[agent_name], reward, next_frame, episode_over)\n",
    "\n",
    "                frame[agent_name] = next_frame\n",
    "                learn(agent_name)\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Update the target network\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * (tau) + target_net_state_dict[key] * (1 - tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "    \n",
    "        # End if any agent has a problem or the max_steps are taken\n",
    "        if episode_over:\n",
    "            reward_history[agent_name].append(total_reward[agent_name])\n",
    "            break\n",
    "\n",
    "    update_plot_multi_agent(episode + 1, max_episodes)\n",
    "\n",
    "update_plot_multi_agent(episode + 1, max_episodes, show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_human = natural_env_v0.parallel_env(render_mode=\"human\", max_cycles=64, num_predators=0, num_prey=1,\n",
    "                                  num_obstacles = 0, num_food=1, num_water=1, num_forests=0)\n",
    "\n",
    "for episode in range(2):\n",
    "    state, info = env_human.reset()\n",
    "\n",
    "    frame = {}\n",
    "    # Get the initial frame for each agent\n",
    "    for agent_name in env_human.agents:\n",
    "        frame[agent_name] = torch.tensor(state[agent_name], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Get the total reward for each agent\n",
    "    episode_over = False\n",
    "\n",
    "    # Keep going\n",
    "    while True:\n",
    "        actions = {}\n",
    "        # Choose actions for each agent\n",
    "        for agent_name in env_human.agents:\n",
    "            action = choose_action(frame[agent_name], env_human.action_space(agent_name), epsilon, explotation=True)\n",
    "            actions[agent_name] = action.item()\n",
    "\n",
    "\n",
    "        # Do all actions at the same time\n",
    "        observation, reward, terminated, truncated, info = env_human.step(actions)\n",
    "\n",
    "        if all(terminated.values()) or all(truncated.values()):\n",
    "            episode_over = True\n",
    "        else:\n",
    "            # Get the results of the actions for each agent\n",
    "            for agent_name in env_human.agents:\n",
    "                # Save the observation in a tensor\n",
    "                frame[agent_name] = torch.tensor(observation[agent_name], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "        # End if any agent has a problem or the max_steps are taken\n",
    "        if episode_over:\n",
    "            break\n",
    "\n",
    "\n",
    "env_human.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
