{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "# Inicializar el entorno\n",
    "env = natural_env_v0.parallel_env(render_mode=None, max_cycles=100, num_predators=0, num_prey=1,\n",
    "                                  num_obstacles=0, num_food=1, num_water=1, num_forests=0)\n",
    "\n",
    "observations, infos = env.reset(seed=42)\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "clip_epsilon = 0.2\n",
    "action_space = 5\n",
    "epsilon = 0.9  # Tasa de exploración elevada\n",
    "\n",
    "# Definir la red neuronal para la política\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # Dropout con probabilidad de 0.3\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout con probabilidad de 0.3\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.fc3(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Crear red para la presa\n",
    "input_dim = observations['prey_0'].shape[0]\n",
    "policy_prey = PolicyNetwork(input_dim, action_space)\n",
    "optimizer_prey = optim.Adam(policy_prey.parameters(), lr=learning_rate)\n",
    "\n",
    "# Funciones de ayuda\n",
    "def get_action(observation, policy_net, exploration=True, epsilon=0.9):\n",
    "    observation = torch.tensor(observation, dtype=torch.float32)\n",
    "    action_probs = policy_net(observation)\n",
    "    action_dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        action = random.randint(0, action_space - 1)\n",
    "        log_prob = torch.log(torch.tensor(1.0 / action_space, dtype=torch.float32))\n",
    "        return action, log_prob\n",
    "\n",
    "    action = action_dist.sample()\n",
    "    return action.item(), action_dist.log_prob(action)\n",
    "\n",
    "def ppo_update(experience, policy_net, optimizer, clip_eps=clip_epsilon):\n",
    "    log_probs_old = []\n",
    "    rewards = []\n",
    "    log_probs_new = []\n",
    "\n",
    "    for state, action, reward, log_prob_old in experience:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        log_prob_old = log_prob_old.clone().detach()\n",
    "\n",
    "        action_probs = policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        log_prob_new = action_dist.log_prob(torch.tensor(action))\n",
    "\n",
    "        log_probs_old.append(log_prob_old)\n",
    "        rewards.append(reward)\n",
    "        log_probs_new.append(log_prob_new)\n",
    "\n",
    "    log_probs_old = torch.stack(log_probs_old)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    log_probs_new = torch.stack(log_probs_new)\n",
    "\n",
    "    advantage = rewards - rewards.mean()\n",
    "\n",
    "    ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "    clipped_ratios = torch.clamp(ratios, 1 - clip_eps, 1 + clip_eps)\n",
    "    loss = -torch.min(ratios * advantage, clipped_ratios * advantage).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Función para identificar puntos óptimos de Pareto\n",
    "def is_pareto_optimal(points):\n",
    "    \"\"\"Devuelve un array booleano indicando si cada punto es óptimo de Pareto.\"\"\"\n",
    "    is_optimal = np.ones(points.shape[0], dtype=bool)\n",
    "    for i, point in enumerate(points):\n",
    "        if is_optimal[i]:\n",
    "            is_optimal[i] = not np.any(np.all(points <= point, axis=1) & np.any(points < point, axis=1))\n",
    "    return is_optimal\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 100\n",
    "rewards_multiobjective = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observations, infos = env.reset()\n",
    "    experience_prey = []\n",
    "    total_reward_resources = 0\n",
    "    total_reward_survival = 0\n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while env.agents:\n",
    "        actions = {}\n",
    "\n",
    "        for agent in env.agents:\n",
    "            obs = observations[agent]\n",
    "            obs = (obs - np.mean(obs)) / (np.std(obs) + 1e-5)\n",
    "            if \"prey\" in agent:\n",
    "                action, log_prob = get_action(obs, policy_prey, epsilon=epsilon)\n",
    "                actions[agent] = action\n",
    "\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        for agent in env.agents:\n",
    "            if \"prey\" in agent:\n",
    "                reward_vector = rewards[agent]\n",
    "                total_reward_resources += reward_vector[0]\n",
    "                total_reward_survival += reward_vector[1]\n",
    "                action = actions[agent]\n",
    "                log_prob = get_action(obs, policy_prey)[1]\n",
    "                experience_prey.append((obs, action, sum(reward_vector), log_prob))\n",
    "\n",
    "        env.agents = [agent for agent in env.agents if not (terminations[agent] or truncations[agent])]\n",
    "\n",
    "    rewards_multiobjective.append([total_reward_resources, total_reward_survival])\n",
    "    ppo_update(experience_prey, policy_prey, optimizer_prey)\n",
    "\n",
    "    epsilon = max(0.1, epsilon * 0.99)\n",
    "\n",
    "    print(f\"Recompensas acumuladas: Recursos={total_reward_resources}, Supervivencia={total_reward_survival}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Visualización del Frente de Pareto\n",
    "rewards_array = np.array(rewards_multiobjective)\n",
    "pareto_optimal = is_pareto_optimal(rewards_array)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rewards_array[:, 0], rewards_array[:, 1], label='No Óptimo de Pareto', alpha=0.5)\n",
    "plt.scatter(rewards_array[pareto_optimal, 0], rewards_array[pareto_optimal, 1], color='red', label='Óptimo de Pareto')\n",
    "plt.xlabel('Recompensa por Recursos (Comida/Agua)')\n",
    "plt.ylabel('Recompensa por Supervivencia')\n",
    "plt.title('Frente de Pareto para la Presa')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
