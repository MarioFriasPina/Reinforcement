{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pettingzoo\n",
      "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ilhui\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pettingzoo) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\ilhui\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pettingzoo) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ilhui\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium>=0.28.0->pettingzoo) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ilhui\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium>=0.28.0->pettingzoo) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\ilhui\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n",
      "Using cached pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
      "Installing collected packages: pettingzoo\n",
      "Successfully installed pettingzoo-1.24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\ilhui\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicio del Episodio 1\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 1\n",
      "Acciones de adversary_0: [0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 4, 0, 1, 0, 1, 0, 0, 3, 0, 0, 2, 1, 1, 0, 3, 0, 4, 2, 4, 0, 1, 0, 2, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 4, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 4, 3, 0, 0, 3, 3, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 3, 0, 4, 0, 2, 0, 0, 0, 0, 0, 4, 4, 4, 0, 4]\n",
      "Acciones de adversary_1: [1, 0, 2, 0, 0, 0, 3, 0, 2, 3, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0, 4, 0, 4, 1, 0, 3, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 2, 4, 4, 3, 1, 3, 0, 1, 0, 0, 4, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 1, 0, 1, 3, 4, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 4, 2, 0, 1, 2, 0, 0, 3, 0, 1]\n",
      "Acciones de adversary_2: [0, 2, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 0, 2, 0, 0, 2, 3, 2, 0, 2, 0, 0, 0, 3, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 3, 4, 1, 2, 0, 1, 2, 3, 0, 0, 1, 0, 0, 0, 4, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 3, 0, 0, 2, 3, 2, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 4, 0, 2, 3, 3, 0, 0, 0, 3, 3, 0, 2, 3, 2, 1, 4, 0, 0, 3]\n",
      "Acciones de agent_0: [0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 2, 4, 4, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 2, 0, 0, 0, 0, 0, 4, 1, 0, 4, 3, 0, 3, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 3, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 3]\n",
      "Recompensas acumuladas: {'adversary_0': 0.0, 'adversary_1': 0.0, 'adversary_2': 0.0, 'agent_0': -267.29699750379166}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "(16, 5)\n",
      "(16, 5)\n",
      "\n",
      "Inicio del Episodio 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m         experience_predator[agent]\u001b[38;5;241m.\u001b[39mappend([state, action, calc_state(observation), reward, termination \u001b[38;5;129;01mor\u001b[39;00m truncation])\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Llamar a env.step() siempre, incluso si el agente ha terminado\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (termination \u001b[38;5;129;01mor\u001b[39;00m truncation):\n\u001b[0;32m    104\u001b[0m     all_terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\assert_out_of_bounds.py:26\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         action\n\u001b[0;32m     25\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction is not in action space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\mpe\\_mpe_utils\\simple_env.py:264\u001b[0m, in \u001b[0;36mSimpleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\pettingzoo\\mpe\\_mpe_utils\\simple_env.py:287\u001b[0m, in \u001b[0;36mSimpleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    286\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "# Inicializar el entorno\n",
    "env = simple_tag_v3.env(render_mode=\"human\", max_cycles=100)\n",
    "env.reset(seed=42)\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "action_space = 5\n",
    "epsilon = 0.5\n",
    "\n",
    "# Definir el total de estados y modelo\n",
    "total_states = (env.observation_space(env.agents[0]).shape[0], action_space)\n",
    "model_params_prey = np.zeros((total_states[0], action_space))\n",
    "model_params_predator = np.zeros((total_states[0], action_space))\n",
    "\n",
    "# Estado interno de las presas\n",
    "prey_state = {\n",
    "    \"energy\": 100,\n",
    "    \"thirst\": 0\n",
    "}\n",
    "\n",
    "# Funciones de ayuda\n",
    "def calc_state(observation):\n",
    "    return int(observation.sum()) % total_states[0]\n",
    "\n",
    "def get_action(state, model_params, exploration=True):\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    else:\n",
    "        return model_params[state].argmax()\n",
    "\n",
    "def train(experience, model_params, lr=learning_rate, df=discount_factor):\n",
    "    \"\"\"Entrena el modelo actualizando parámetros usando Q-learning.\"\"\"\n",
    "    for prev_state, action_taken, state, reward, final in reversed(experience):\n",
    "        # Actualización de Q-learning usando la recompensa del entorno\n",
    "        best_next_action = model_params[state].max()\n",
    "        target = reward + df * best_next_action\n",
    "        model_params[prev_state][action_taken] += lr * (target - model_params[prev_state][action_taken])\n",
    "\n",
    "# Lógica para actualizar el estado interno de las presas\n",
    "def update_prey_state(prey_state, action):\n",
    "    if action == 1:  # Buscar agua\n",
    "        prey_state[\"thirst\"] = max(0, prey_state[\"thirst\"] - 10)\n",
    "    elif action == 2:  # Buscar comida\n",
    "        prey_state[\"energy\"] = min(100, prey_state[\"energy\"] + 10)\n",
    "    elif action == 0:  # Huir\n",
    "        prey_state[\"energy\"] -= 5\n",
    "        prey_state[\"thirst\"] += 2\n",
    "    else:  # Descansar\n",
    "        prey_state[\"energy\"] += 1\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 10\n",
    "prey_survival_times = {}\n",
    "agent_actions = {}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    experience_prey = {agent: [] for agent in env.agents if \"agent\" in agent}\n",
    "    experience_predator = {agent: [] for agent in env.agents if \"adversary\" in agent}\n",
    "    survival_time = {agent: 0 for agent in env.agents if \"agent\" in agent}\n",
    "    actions_log = {agent: [] for agent in env.agents}\n",
    "    total_rewards = {agent: 0 for agent in env.agents}\n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while True:\n",
    "        all_terminated = True\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Registrar la recompensa obtenida\n",
    "            total_rewards[agent] += reward\n",
    "\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                print(f\"{agent} ha terminado y no toma acción.\")\n",
    "            else:\n",
    "                state = calc_state(observation)\n",
    "                if \"agent\" in agent:  # Presa\n",
    "                    action = get_action(state, model_params_prey)\n",
    "                    update_prey_state(prey_state, action)\n",
    "                    survival_time[agent] += 1  # Incrementar el tiempo de supervivencia de la presa\n",
    "                elif \"adversary\" in agent:  # Depredador\n",
    "                    action = get_action(state, model_params_predator)\n",
    "\n",
    "                # Registrar la acción tomada por el agente\n",
    "                actions_log[agent].append(action)\n",
    "\n",
    "                # Determinar si es presa o depredador y guardar la experiencia\n",
    "                if \"agent\" in agent:\n",
    "                    experience_prey[agent].append([state, action, calc_state(observation), reward, termination or truncation])\n",
    "                else:\n",
    "                    experience_predator[agent].append([state, action, calc_state(observation), reward, termination or truncation])\n",
    "\n",
    "            # Llamar a env.step() siempre, incluso si el agente ha terminado\n",
    "            env.step(action)\n",
    "\n",
    "            if not (termination or truncation):\n",
    "                all_terminated = False\n",
    "\n",
    "        if all_terminated:\n",
    "            break\n",
    "\n",
    "    # Entrenar al final del episodio\n",
    "    for agent in experience_prey:\n",
    "        train(experience_prey[agent], model_params_prey)\n",
    "    for agent in experience_predator:\n",
    "        train(experience_predator[agent], model_params_predator)\n",
    "\n",
    "    # Guardar el tiempo de supervivencia de las presas\n",
    "    for agent in survival_time:\n",
    "        if agent not in prey_survival_times:\n",
    "            prey_survival_times[agent] = []\n",
    "        prey_survival_times[agent].append(survival_time[agent])\n",
    "\n",
    "    # Mostrar las recompensas acumuladas y acciones realizadas\n",
    "    print(f\"\\nResumen del Episodio {episode + 1}\")\n",
    "    for agent, actions in actions_log.items():\n",
    "        print(f\"Acciones de {agent}: {actions}\")\n",
    "\n",
    "    print(f\"Recompensas acumuladas: {total_rewards}\")\n",
    "    print(f\"Tiempo de supervivencia de las presas: {survival_time}\")\n",
    "    print(model_params_predator.shape)\n",
    "    print(model_params_prey.shape)\n",
    "\n",
    "print(\"\\nResultados Finales:\")\n",
    "for agent, times in prey_survival_times.items():\n",
    "    promedio = np.mean(times)\n",
    "    print(f\"Presa {agent} sobrevivió en promedio {promedio:.2f} pasos.\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
