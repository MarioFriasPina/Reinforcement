{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pettingzoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicio del Episodio 1\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 1\n",
      "Recompensas acumuladas: {'adversary_0': 28, 'adversary_1': 20, 'adversary_2': 28, 'agent_0': -40}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 2\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 2\n",
      "Recompensas acumuladas: {'adversary_0': 38, 'adversary_1': 56, 'adversary_2': 26, 'agent_0': -100}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 3\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 3\n",
      "Recompensas acumuladas: {'adversary_0': 132, 'adversary_1': 124, 'adversary_2': 102, 'agent_0': -100}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 4\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 4\n",
      "Recompensas acumuladas: {'adversary_0': 108, 'adversary_1': 124, 'adversary_2': 100, 'agent_0': -100}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 5\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 5\n",
      "Recompensas acumuladas: {'adversary_0': 146, 'adversary_1': 116, 'adversary_2': 142, 'agent_0': 21}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 6\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 6\n",
      "Recompensas acumuladas: {'adversary_0': 124, 'adversary_1': 110, 'adversary_2': 120, 'agent_0': 39}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 7\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 7\n",
      "Recompensas acumuladas: {'adversary_0': 126, 'adversary_1': 122, 'adversary_2': 130, 'agent_0': 53}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 8\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 8\n",
      "Recompensas acumuladas: {'adversary_0': 104, 'adversary_1': 142, 'adversary_2': 118, 'agent_0': 46}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 9\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 9\n",
      "Recompensas acumuladas: {'adversary_0': 110, 'adversary_1': 116, 'adversary_2': 116, 'agent_0': 57}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Inicio del Episodio 10\n",
      "adversary_0 ha terminado y no toma acción.\n",
      "adversary_1 ha terminado y no toma acción.\n",
      "adversary_2 ha terminado y no toma acción.\n",
      "agent_0 ha terminado y no toma acción.\n",
      "\n",
      "Resumen del Episodio 10\n",
      "Recompensas acumuladas: {'adversary_0': 136, 'adversary_1': 102, 'adversary_2': 112, 'agent_0': 61}\n",
      "Tiempo de supervivencia de las presas: {'agent_0': 100}\n",
      "\n",
      "Resultados Finales:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "# Inicializar el entorno\n",
    "env = simple_tag_v3.env(render_mode=None, max_cycles=100)\n",
    "env.reset()\n",
    "\n",
    "# Parámetros del aprendizaje\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "action_space = 5\n",
    "epsilon = 0.5\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "# Definir el total de estados y modelo\n",
    "total_states = (env.observation_space(env.agents[0]).shape[0], action_space)\n",
    "policy_prey = np.zeros((total_states[0], action_space))\n",
    "policy_predator = np.zeros((total_states[0], action_space))\n",
    "old_policy_prey = policy_prey.copy()\n",
    "old_policy_predator = policy_predator.copy()\n",
    "\n",
    "# Estado interno de las presas\n",
    "prey_state = {\n",
    "    \"energy\": 100,\n",
    "    \"thirst\": 0\n",
    "}\n",
    "\n",
    "# Funciones de ayuda\n",
    "def calc_state(observation):\n",
    "    return int(observation.sum()) % total_states[0]\n",
    "\n",
    "def get_action(state, model_params, exploration=True):\n",
    "    if exploration and random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(action_space)\n",
    "    else:\n",
    "        return model_params[state].argmax()\n",
    "\n",
    "def update_prey_state(prey_state, action):\n",
    "    if action == 1:  # Buscar agua\n",
    "        prey_state[\"thirst\"] = max(0, prey_state[\"thirst\"] - 10)\n",
    "    elif action == 2:  # Buscar comida\n",
    "        prey_state[\"energy\"] = min(100, prey_state[\"energy\"] + 10)\n",
    "    elif action == 0:  # Huir\n",
    "        prey_state[\"energy\"] -= 5\n",
    "        prey_state[\"thirst\"] += 2\n",
    "    else:  # Descansar\n",
    "        prey_state[\"energy\"] += 1\n",
    "\n",
    "def calculate_reward(agent, prey_state, action, termination):\n",
    "    if \"agent\" in agent:  # Presa\n",
    "        if termination:\n",
    "            return -10  # Muerte\n",
    "        if prey_state[\"energy\"] <= 0 or prey_state[\"thirst\"] >= 100:\n",
    "            return -1  # Hambre o sed\n",
    "        if action in [1, 2]:\n",
    "            return 1  # Encontrar comida o agua\n",
    "        return 0\n",
    "    elif \"adversary\" in agent:  # Depredador\n",
    "        if termination:\n",
    "            return -10  # Muerte\n",
    "        if action == 3:  # Comer presa\n",
    "            return 2\n",
    "        return 0\n",
    "\n",
    "def compute_advantage(rewards, values):\n",
    "    return rewards - values\n",
    "\n",
    "def ppo_update(experience, model_params, old_model_params, lr=learning_rate, clip_eps=clip_epsilon):\n",
    "    \"\"\"Actualiza el modelo utilizando PPO.\"\"\"\n",
    "    for prev_state, action, state, reward, final in reversed(experience):\n",
    "        # Calcular ventaja y normalizar\n",
    "        advantage = compute_advantage(reward, model_params[prev_state][action])\n",
    "        advantage = np.clip(advantage, -10, 10)  # Evitar valores extremos\n",
    "\n",
    "        # Calcular el ratio y aplicar corrección numérica\n",
    "        ratio = model_params[prev_state][action] / (old_model_params[prev_state][action] + 1e-5)\n",
    "        ratio = np.clip(ratio, 0.1, 10)  # Limitar el ratio\n",
    "\n",
    "        # Aplicar clipping al ratio\n",
    "        clipped_ratio = np.clip(ratio, 1 - clip_eps, 1 + clip_eps)\n",
    "\n",
    "        # Calcular la pérdida y actualizar los parámetros\n",
    "        loss = -min(ratio * advantage, clipped_ratio * advantage)\n",
    "        model_params[prev_state][action] -= lr * loss\n",
    "\n",
    "# Bucle de episodios\n",
    "num_episodes = 10\n",
    "prey_survival_times = {}\n",
    "agent_actions = {}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    experience_prey = {agent: [] for agent in env.agents if \"agent\" in agent}\n",
    "    experience_predator = {agent: [] for agent in env.agents if \"adversary\" in agent}\n",
    "    survival_time = {agent: 0 for agent in env.agents if \"agent\" in agent}\n",
    "    actions_log = {agent: [] for agent in env.agents}\n",
    "    total_rewards = {agent: 0 for agent in env.agents}\n",
    "\n",
    "    print(f\"\\nInicio del Episodio {episode + 1}\")\n",
    "\n",
    "    while True:\n",
    "        all_terminated = True\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                print(f\"{agent} ha terminado y no toma acción.\")\n",
    "            else:\n",
    "                state = calc_state(observation)\n",
    "                if \"agent\" in agent:  # Presa\n",
    "                    action = get_action(state, policy_prey)\n",
    "                    update_prey_state(prey_state, action)\n",
    "                    survival_time[agent] += 1\n",
    "                elif \"adversary\" in agent:  # Depredador\n",
    "                    action = get_action(state, policy_predator)\n",
    "\n",
    "                # Calcular recompensa personalizada\n",
    "                custom_reward = calculate_reward(agent, prey_state, action, termination)\n",
    "                total_rewards[agent] += custom_reward\n",
    "\n",
    "                # Registrar la acción tomada y la experiencia\n",
    "                actions_log[agent].append(action)\n",
    "                if \"agent\" in agent:\n",
    "                    experience_prey[agent].append([state, action, calc_state(observation), custom_reward, termination or truncation])\n",
    "                else:\n",
    "                    experience_predator[agent].append([state, action, calc_state(observation), custom_reward, termination or truncation])\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "            if not (termination or truncation):\n",
    "                all_terminated = False\n",
    "\n",
    "        if all_terminated:\n",
    "            break\n",
    "\n",
    "    # Actualizar la política utilizando PPO\n",
    "    for agent in experience_prey:\n",
    "        ppo_update(experience_prey[agent], policy_prey, old_policy_prey)\n",
    "    for agent in experience_predator:\n",
    "        ppo_update(experience_predator[agent], policy_predator, old_policy_predator)\n",
    "\n",
    "    # Copiar política actual a política antigua\n",
    "    old_policy_prey = policy_prey.copy()\n",
    "    old_policy_predator = policy_predator.copy()\n",
    "\n",
    "    print(f\"\\nResumen del Episodio {episode + 1}\")\n",
    "    print(f\"Recompensas acumuladas: {total_rewards}\")\n",
    "    print(f\"Tiempo de supervivencia de las presas: {survival_time}\")\n",
    "\n",
    "print(\"\\nResultados Finales:\")\n",
    "for agent, times in prey_survival_times.items():\n",
    "    promedio = np.mean(times)\n",
    "    print(f\"Presa {agent} sobrevivió en promedio {promedio:.2f} pasos.\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
