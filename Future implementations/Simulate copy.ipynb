{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:08:04,345 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "scenario = \"navigation\"\n",
    "#scenario = \"sampling\"\n",
    "#scenario = \"reverse_transport\"\n",
    "#scenario = \"balance\"\n",
    "\n",
    "max_steps = 512  # Episode steps before done\n",
    "\n",
    "env = VmasEnv(\n",
    "    num_envs=1,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "\n",
    "    scenario = scenario\n",
    ")\n",
    "\n",
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")\n",
    "\n",
    "check_env_specs(env)\n",
    "\n",
    "rollout = env.rollout(4)\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=2 * env.action_spec.shape[-1],  # 2 * n_actions_per_agents\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=True,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    ")\n",
    "\n",
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.unbatched_action_spec,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "        \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")  # we'll need the log-prob for the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6033/2243165039.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.load_state_dict(torch.load(f\"models/{scenario}_policy.pth\"))\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: TypeError: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m    \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_cast_to_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Reinforcement/Reinforcement_env/lib/python3.10/site-packages/torchrl/envs/common.py:2641\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[0m\n\u001b[1;32m   2635\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_stop_early(\n\u001b[1;32m   2636\u001b[0m         break_when_all_done\u001b[38;5;241m=\u001b[39mbreak_when_all_done,\n\u001b[1;32m   2637\u001b[0m         break_when_any_done\u001b[38;5;241m=\u001b[39mbreak_when_any_done,\n\u001b[1;32m   2638\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2639\u001b[0m     )\n\u001b[1;32m   2640\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2641\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_nonstop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2642\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n",
      "File \u001b[0;32m~/Reinforcement/Reinforcement_env/lib/python3.10/site-packages/torchrl/envs/common.py:2814\u001b[0m, in \u001b[0;36mEnvBase._rollout_nonstop\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   2812\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2814\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensordicts\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(env, _)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m    \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     env\u001b[38;5;241m.\u001b[39mrollout(\n\u001b[1;32m      7\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39mmax_steps,\n\u001b[1;32m      8\u001b[0m         policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m----> 9\u001b[0m         callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m env, _: \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     10\u001b[0m         auto_cast_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         break_when_any_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     )\n",
      "File \u001b[0;32m~/Reinforcement/Reinforcement_env/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:792\u001b[0m, in \u001b[0;36mEnvironment.render\u001b[0;34m(self, mode, env_index, agent_index_focus, visualize_when_rgb, plot_position_function, plot_position_function_precision, plot_position_function_range, plot_position_function_cmap_range, plot_position_function_cmap_alpha, plot_position_function_cmap_name)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39madd_onetime_list(entity\u001b[38;5;241m.\u001b[39mrender(env_index\u001b[38;5;241m=\u001b[39menv_index))\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# render to display or array\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Reinforcement/Reinforcement_env/lib/python3.10/site-packages/vmas/simulator/rendering.py:149\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m    153\u001b[0m text_lines \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Reinforcement/Reinforcement_env/lib/python3.10/site-packages/pyglet/window/xlib/__init__.py:939\u001b[0m, in \u001b[0;36mXlibWindow.dispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m _view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Check for the events specific to this window\u001b[39;00m\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mxlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXCheckWindowEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x_display\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x1ffffff\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# Key events are filtered by the xlib window event\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# handler so they get a shot at the prefiltered event.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mxany\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (xlib\u001b[38;5;241m.\u001b[39mKeyPress, xlib\u001b[38;5;241m.\u001b[39mKeyRelease):\n\u001b[1;32m    943\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xlib\u001b[38;5;241m.\u001b[39mXFilterEvent(e, \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: TypeError: wrong type"
     ]
    }
   ],
   "source": [
    "policy.load_state_dict(torch.load(f\"models/{scenario}_policy.pth\"))\n",
    "\n",
    "# Run the trained policy on the test environment\n",
    "with torch.no_grad():\n",
    "   while True:\n",
    "    env.rollout(\n",
    "        max_steps=max_steps,\n",
    "        policy=policy,\n",
    "        callback=lambda env, _: env.render(),\n",
    "        auto_cast_to_device=True,\n",
    "        break_when_any_done=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
