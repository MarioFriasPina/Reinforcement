{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "Observation spaces: {'prey_0': 16}\n",
      "Action spaces: {'prey_0': 5}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "\n",
    "from NaturalEnv import natural_env_v0\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = {\n",
    "    'render_mode': None,\n",
    "    'max_cycles': 256,\n",
    "    'continuous_actions': True,\n",
    "    'num_predators': 0,\n",
    "    'num_prey': 1,\n",
    "    'num_obstacles': 0,\n",
    "    'num_food': 1,\n",
    "    'num_water': 1,\n",
    "    'num_forests': 0\n",
    "}\n",
    "\n",
    "env = natural_env_v0.parallel_env(**args)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Get observation and action spaces\n",
    "obs_spaces = {agent: env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_spaces = {agent: env.action_space(agent).shape[0] for agent in env.agents}\n",
    "agents = env.agents  # List of agents\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"Observation spaces: {obs_spaces}\")\n",
    "print(f\"Action spaces: {action_spaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        input_dim = obs_dim + action_dim  # Critic gets all obs & actions\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs, actions):\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "class ParallelReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, observations, actions, rewards, next_observations, dones):\n",
    "        # Store a single transition for all agents\n",
    "        self.buffer.append((observations, actions, rewards, next_observations, dones))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        \n",
    "        # Convert dictionary data into tensors\n",
    "        obs = {agent: torch.tensor(np.array([b[agent] for b in obs]), dtype=torch.float32, device=device) for agent in obs[0]}\n",
    "        actions = {agent: torch.tensor(np.array([b[agent] for b in actions]), dtype=torch.float32, device=device) for agent in actions[0]}\n",
    "        rewards = {agent: torch.tensor([b[agent] for b in rewards], dtype=torch.float32, device=device) for agent in rewards[0]}\n",
    "        next_obs = {agent: torch.tensor(np.array([b[agent] for b in next_obs]), dtype=torch.float32, device=device) for agent in next_obs[0]}\n",
    "        dones = {agent: torch.tensor([b[agent] for b in dones], dtype=torch.float32, device=device) for agent in dones[0]}\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(actors, critics, target_actors, target_critics, buffer, batch_size, gamma=0.95, tau=0.01):\n",
    "    obs, actions, rewards, next_obs, dones = buffer.sample(batch_size)\n",
    "    \n",
    "    # Centralized Q-value update for each agent\n",
    "    for i, agent in enumerate(agents):\n",
    "        # Get target actions for all agents\n",
    "        target_actions = torch.cat([target_actors[j](next_obs[other]) for j, other in enumerate(agents)], dim=-1)\n",
    "        obs_concat = torch.cat([obs[other] for other in agents], dim=-1)\n",
    "        next_obs_concat = torch.cat([next_obs[other] for other in agents], dim=-1)\n",
    "        \n",
    "        # Compute target Q-value\n",
    "        target_q = target_critics[i](next_obs_concat, target_actions).detach()\n",
    "        y = rewards[agent] + gamma * (1 - dones[agent]) * target_q.squeeze()\n",
    "        \n",
    "        # Predicted Q-value\n",
    "        actions_concat = torch.cat([actions[other] for other in agents], dim=-1)\n",
    "        current_q = critics[i](obs_concat, actions_concat).squeeze()\n",
    "        \n",
    "        # Critic Loss\n",
    "        critic_loss = torch.nn.functional.mse_loss(current_q, y)\n",
    "        critics[i].optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critics[i].optimizer.step()\n",
    "\n",
    "    # Policy (actor) update\n",
    "    for i, agent in enumerate(agents):\n",
    "        current_actions = torch.cat(\n",
    "            [actors[j](obs[other]) if other == agent else actions[other].detach() for j, other in enumerate(agents)], dim=-1\n",
    "        )\n",
    "        actor_loss = -critics[i](obs_concat, current_actions).mean()\n",
    "        actors[i].optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actors[i].optimizer.step()\n",
    "\n",
    "    # Soft update for target networks\n",
    "    for i, agent in enumerate(agents):\n",
    "        for target_param, param in zip(target_critics[i].parameters(), critics[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(target_actors[i].parameters(), actors[i].parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "def update_plot_multi_agent(episode, max_episodes, reward_history, show_result=False):\n",
    "    plt.figure(1)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title(f'Final Result:')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "\n",
    "    for agent_name in agents:\n",
    "        agent_name = agents[0]\n",
    "        rewards_t = torch.tensor(reward_history[agent_name], dtype=torch.float)\n",
    "        plt.plot(rewards_t.numpy(), label=agent_name)\n",
    "\n",
    "        # Plot moving average of last 10 rewards\n",
    "        if len(rewards_t) >= 10:\n",
    "            means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(9), means))\n",
    "            plt.plot(means.numpy())            \n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Choose actions for each agent\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43magent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mactors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     next_obs, rewards, terminated, truncated,  _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Choose actions for each agent\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent: actors[i](\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agents)}\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     next_obs, rewards, terminated, truncated,  _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize actors, critics, target networks, and optimizers\n",
    "actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "target_actors = [Actor(obs_spaces[agent], action_spaces[agent]).to(device) for agent in env.agents]\n",
    "target_critics = [Critic(sum(obs_spaces.values()), sum(action_spaces.values())).to(device) for _ in env.agents]\n",
    "\n",
    "# Optimizers\n",
    "for actor, critic in zip(actors, critics):\n",
    "    actor.optimizer = Adam(actor.parameters(), lr=1e-3)\n",
    "    critic.optimizer = Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = ParallelReplayBuffer()\n",
    "\n",
    "# Main training loop\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "plt.ion()\n",
    "reward_history = {agent_name: [] for agent_name in agents}\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Get initial observations\n",
    "    done = defaultdict(bool, {agent: False for agent in agents})\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "    \n",
    "    while not all(done.values()):\n",
    "        # Choose actions for each agent\n",
    "        actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, rewards, terminated, truncated,  _ = env.step(actions)\n",
    "\n",
    "        # Compute reward for each agent\n",
    "        for agent in agents:\n",
    "            episode_reward[agent] += rewards[agent]\n",
    "\n",
    "        # Update done flag\n",
    "        done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "                \n",
    "        # Store transition in replay buffer\n",
    "        buffer.add(obs, actions, rewards, next_obs, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        # Training step if enough data in buffer\n",
    "        if buffer.size() > batch_size:\n",
    "            train_step(actors, critics, target_actors, target_critics, buffer, batch_size)\n",
    "\n",
    "    reward_history = {agent_name: reward_history[agent_name] + [episode_reward[agent_name]] for agent_name in agents}\n",
    "    update_plot_multi_agent(episode, episodes, reward_history)\n",
    "\n",
    "update_plot_multi_agent(episode + 1, episodes, reward_history, show_result=True)\n",
    "plt.ioff()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "human_env = natural_env_v0.parallel_env(render_mode=\"human\", continuous_actions=True, **args)\n",
    "\n",
    "obs, _ = human_env.reset()\n",
    "done = defaultdict(bool, {agent: False for agent in agents})\n",
    "\n",
    "while not all(done.values()):\n",
    "    actions = {agent: actors[i](torch.tensor(obs[agent], dtype=torch.float32, device=device)).cpu().detach().numpy() for i, agent in enumerate(agents)}\n",
    "    obs, _, terminated, truncated, _ = human_env.step(actions)\n",
    "    done = defaultdict(bool, {agent: terminated[agent] or truncated[agent] for agent in agents})\n",
    "\n",
    "human_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
