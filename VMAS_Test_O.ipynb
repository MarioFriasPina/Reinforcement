{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "import random\n",
    "\n",
    "# Crear el entorno utilizando el escenario \"navigation\"\n",
    "env = vmas.make_env(\n",
    "    scenario=\"navigation\",  # Cambiamos a \"navigation\"\n",
    "    num_envs=1,             # Número de entornos paralelos\n",
    "    device=\"cuda\",          # Cambiar a \"cpu\" si no tienes GPU disponible\n",
    "    continuous_actions=True,  # True para acciones continuas\n",
    "    wrapper=None,            # Ningún wrapper por defecto\n",
    "    max_steps=100,           # Define el horizonte, 100 pasos como ejemplo\n",
    "    seed=42,                 # Seed para reproducibilidad\n",
    "    dict_spaces=False,       # Espacio de observación y acción como tuplas\n",
    "    grad_enabled=False,      # Gradientes deshabilitados\n",
    "    terminated_truncated=False,  # Utiliza una sola bandera `done`\n",
    ")\n",
    "\n",
    "# Ciclo de interacción con el entorno\n",
    "for _ in range(100):  # Número de episodios o iteraciones\n",
    "    dict_actions = random.choice([True, False])  # Decidir si usar diccionario de acciones\n",
    "    actions = {} if dict_actions else []  # Inicializar las acciones\n",
    "\n",
    "    # Generar acciones aleatorias para cada agente\n",
    "    for agent in env.agents:\n",
    "        action = env.get_random_action(agent)\n",
    "        if dict_actions:\n",
    "            actions.update({agent.name: action})\n",
    "        else:\n",
    "            actions.append(action)\n",
    "    \n",
    "    # Paso en el entorno\n",
    "    obs, rew, dones, info = env.step(actions)\n",
    "\n",
    "    # Renderizado del entorno\n",
    "    frame = env.render(\n",
    "        mode=\"rgb_array\",\n",
    "        agent_index_focus=None,  # Enfocar la cámara en un agente (opcional)\n",
    "        visualize_when_rgb=True,  # Mostrar visualización al obtener un array RGB\n",
    "    )\n",
    "\n",
    "    # Por si queremos guardar o procesar el frame\n",
    "    # Ejemplo: guardar el frame como imagen usando OpenCV\n",
    "    # import cv2\n",
    "    # cv2.imwrite(f\"frame_{_}.png\", frame)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar recursos del entorno\n",
    "env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents in the environment: 4\n",
      "Observation space: 18, Action space: 9\n",
      "Step Rewards: [-0.0097459555, -0.01399377, -0.0022329092, -0.0105564, -0.023678005, -0.022732258, -0.032022506, -0.015824974, -0.004936695, 0.0052693486, 0.028895855, 0.03597212, 0.045841753, 0.03301543, 0.022581816, 0.023717165, 0.022160023, -0.00876537, -0.0059238374, -0.00486663, -0.018841296, -0.026874304, -0.021352679, -0.014106929, -0.021628201, -0.0040718317, 0.0048449337, 0.022208095, 0.017916083, 0.043294966, 0.035018504, 0.03555295, 0.022949576, 0.02510342, 0.015737593, -0.0065447986, -0.011897653, -0.024578363, -0.016358167, 0.007540822, 0.009846449, -0.010180056, -0.015475541, -0.00916025, -0.011279106, 0.008201152, 0.0059110224, -0.0019500852, 0.0060248375, -0.0019334555, -0.009509653, 0.021449268, 0.02759415, 0.027293652, 0.01602298, 0.011453897, 0.0065059215, -0.0054557174, -0.0060449988, -0.0038965344, 0.0014055073, -0.02894713, -0.027496606, -0.019444183, -0.030253023, -0.041158304, -0.05729541, -0.025908247, -0.0029665828, 0.0030525923, -0.012557551, -0.011135876, 0.007697664, -0.008263633, -0.00019925833, -0.0041585267, 0.0010284409, 0.010158151, -0.008945756, -0.015287243, -0.025802627, -0.018319175, -0.013023585, -0.010864541, -0.007841155, -0.004523322, -0.0063637793, -0.015728548, -0.01646918, -0.014226273, -0.0067264587, -0.0040578693, -0.008408248, -0.022369504, -0.012861162, -0.0072271526, -0.04192713, -0.058386624, -0.056031823, -0.072047144, -0.06450176, -0.055370808, -0.060142696, -0.059699178, -0.05392897, -0.057699203, -0.042508364, -0.033294737, -0.017206669, -0.02308911, 0.0034367442, 0.0126745105, 0.012108743, -0.006209314, -0.010365486, -0.02482307, -0.025740504, -0.01885271, 0.004369378, 0.008927643, 0.02520138, 0.0032846332, -0.008654892, 0.011905491, -0.012448192, -0.023743749, -0.003891468, -0.014596164, -0.0032693744, -0.00086039305, -0.00038427114, -0.009296119, -0.0044596195, -0.008397877, -0.0040483475, 0.005978048, 0.010784209, 0.012769401, 0.02827835, 0.013170838, -0.0053613186, -0.005950272, -0.008674681, 0.005821109, -0.00984782, -0.033058107, -0.024021626, -0.0149615705, 0.008242875, 0.015906215, 0.018434107, 0.008321404, -0.013739824, -0.037504673, -0.021734118, -0.025215209, -0.040597796, -0.046194732, -0.04218322, -0.040051937, -0.03358555, -0.025618255, -0.030583799, -0.03233373, -0.031903923, -0.016897678, -0.010296941, -0.019412756, 0.007280171, 0.004408419, -0.025883257, -0.02689135, -0.036828876, -0.028912246, -0.02131188, -0.013456106, -0.010446668, 0.0022933483, 0.025715709, 0.013107002, 0.007063508, -0.018870115, -0.020369709, -0.0070831776, 0.0024427176, 0.011239886, 0.031586587, 0.045683026, 0.05696827, 0.048618734, 0.031560004, 0.0110013485, 0.015828907, -0.0012253523, 0.0050370097, 0.027961314, 0.035042167, 0.004869938, 0.001406908, 0.0083245635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([800, 1])) that is different to the input size (torch.Size([200, 4, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 167\u001b[0m\n\u001b[0;32m    158\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_ppo_loss(\n\u001b[0;32m    159\u001b[0m         old_log_probs\u001b[38;5;241m.\u001b[39mdetach(),  \u001b[38;5;66;03m# Desacoplar del grafo\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         new_log_probs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         returns\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Desacoplar retornos\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# Retropropagación\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    168\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Imprimir recompensa promedio por época\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ilhui\\anaconda3\\envs\\reto\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import vmas\n",
    "\n",
    "# Configuración del entorno\n",
    "env = vmas.make_env(\n",
    "    scenario=\"navigation\",\n",
    "    num_envs=1,\n",
    "    device=\"cpu\",  # Cambiar a \"cuda\" si queremos usar GPU\n",
    "    continuous_actions=False,  # Ahora trabajamos con acciones discretas\n",
    "    max_steps=200,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Hiperparámetros de PPO\n",
    "GAMMA = 0.99          # Factor de descuento\n",
    "LR = 1e-3             # Tasa de aprendizaje\n",
    "EPS_CLIP = 0.2        # Rango de clipping en PPO\n",
    "ENTROPY_COEF = 0.02   # Coeficiente para la entropía\n",
    "VALUE_LOSS_COEF = 0.5 # Coeficiente para la pérdida del crítico\n",
    "BATCH_SIZE = 64       # Tamaño del batch\n",
    "EPOCHS = 4            # Épocas para actualizar la política\n",
    "\n",
    "# Obtener lista de agentes en el entorno\n",
    "agents = env.agents\n",
    "\n",
    "# Número de agentes\n",
    "num_agents = len(agents)\n",
    "\n",
    "print(f\"Number of agents in the environment: {num_agents}\")\n",
    "\n",
    "\n",
    "# Definición de la red Actor-Crítico\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(64, action_dim)  # Actor (logits para acciones discretas)\n",
    "        self.critic = nn.Linear(64, 1)         # Crítico (valor del estado)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layers(x)\n",
    "        return self.actor(shared), self.critic(shared)\n",
    "\n",
    "# Inicialización\n",
    "# Reseteamos el entorno para obtener las primeras observaciones\n",
    "initial_obs = env.reset()\n",
    "\n",
    "# Determinar dimensiones de observaciones y acciones\n",
    "obs_space = env.observation_space[0].shape[0]  # Primer agente\n",
    "action_space = env.action_space[0].n           # Primer agente\n",
    "\n",
    "# Imprimir las dimensiones para verificar\n",
    "print(f\"Observation space: {obs_space}, Action space: {action_space}\")\n",
    "\n",
    "policy = ActorCritic(obs_space, action_space).to(\"cpu\")\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "\n",
    "# Función de PPO\n",
    "def compute_ppo_loss(old_log_probs, new_log_probs, advantages, values, returns):\n",
    "    # Clipped Surrogate Objective\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP)\n",
    "    policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "    # Value Function Loss\n",
    "    value_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "    # Entropy Loss (para explorar más)\n",
    "    entropy_loss = -new_log_probs.mean()\n",
    "\n",
    "    # Loss final\n",
    "    return policy_loss + VALUE_LOSS_COEF * value_loss - ENTROPY_COEF * entropy_loss\n",
    "\n",
    "\n",
    "def collect_trajectories(policy, env, gamma):\n",
    "    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    state = env.reset()  # state es una lista o tupla con observaciones por agente\n",
    "    total_reward = 0  # Acumular las recompensas totales\n",
    "    step_rewards = []  # Recompensas individuales por paso\n",
    "\n",
    "    for _ in range(env.max_steps):\n",
    "        # Convertir las observaciones de todos los agentes a tensores\n",
    "        state_tensor = torch.tensor(np.array(state), dtype=torch.float32)\n",
    "        logits, value = policy(state_tensor)  # Predicción de la red\n",
    "\n",
    "        # Selección de acciones discretas con probabilidad\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action_distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        \n",
    "        # Interactuar con el entorno\n",
    "        next_state, reward, done, _ = env.step(action.tolist())\n",
    "        \n",
    "        # Guardar las transiciones\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32))  # Guardar recompensas por agente\n",
    "        dones.append(done.clone().detach().to(dtype=torch.float32))  # Guardar banderas de finalización\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "\n",
    "        # Acumular recompensa y guardar por paso\n",
    "        step_rewards.append(np.mean(reward))  # Promedio de recompensas por agente\n",
    "        total_reward += np.mean(reward)\n",
    "\n",
    "        if all(done):  # Si todos los agentes han terminado\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    # Calcular retornos y ventajas\n",
    "    returns = []\n",
    "    G = torch.zeros_like(rewards[0])  # Inicializar con ceros del tamaño de la recompensa por agente\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        G = r + gamma * G * (1 - d)  # Calcular retorno considerando las banderas `done`\n",
    "        returns.insert(0, G)         # Insertar en el inicio para mantener el orden correcto\n",
    "\n",
    "    returns = torch.cat(returns)  # Convertir lista de tensores a un tensor\n",
    "    advantages = returns - torch.cat(values).detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Imprimir recompensas paso a paso para depuración\n",
    "    print(f\"Step Rewards: {step_rewards}\")\n",
    "\n",
    "    # Retornar estados, acciones, retornos, log_probs, ventajas y recompensa total\n",
    "    return states, actions, returns, log_probs, advantages, total_reward / len(step_rewards)\n",
    "\n",
    "\n",
    "\n",
    "# Entrenamiento PPO\n",
    "for epoch in range(1000):\n",
    "    # Recolectar datos\n",
    "    states, actions, returns, old_log_probs, advantages, total_reward = collect_trajectories(policy, env, GAMMA)\n",
    "\n",
    "    # Convertir a tensores\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    returns = returns.unsqueeze(1)\n",
    "    old_log_probs = torch.stack(old_log_probs)\n",
    "    advantages = advantages.unsqueeze(1)\n",
    "\n",
    "    # Actualizar política y crítico\n",
    "    for _ in range(EPOCHS):\n",
    "        optimizer.zero_grad()  # Reiniciar gradientes antes de cada actualización\n",
    "        logits, values = policy(states)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        new_log_probs = torch.gather(torch.log(action_probs), dim=-1, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Calcular pérdida\n",
    "        loss = compute_ppo_loss(\n",
    "            old_log_probs.detach(),  # Desacoplar del grafo\n",
    "            new_log_probs,\n",
    "            advantages.detach(),  # Desacoplar ventajas para evitar acumulación de gradientes\n",
    "            values,\n",
    "            returns.detach()  # Desacoplar retornos\n",
    "        )\n",
    "\n",
    "        # Retropropagación\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Imprimir recompensa promedio por época\n",
    "    print(f\"Epoch {epoch}: Average Reward = {total_reward/env.max_steps:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
