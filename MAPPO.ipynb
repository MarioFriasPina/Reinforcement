{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "is_fork = multiprocessing.get_start_method() == 'fork'\n",
    "device = (\n",
    "    torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device('cpu')\n",
    ")\n",
    "vmas_device = device\n",
    "\n",
    "# Environment\n",
    "scenario_name = \"navigation\"\n",
    "\n",
    "num_epochs = 1024  # Total number of training epochs, i.e. number of steps\n",
    "max_steps = 128  # Episode steps before reset\n",
    "num_vmas_envs = 1  # Number of vectorized environments to simulate at once\n",
    "\n",
    "n_agents = 1 # Number of agents in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action spaces: Dict('agent_0': Discrete(9))\n",
      "Observation spaces: Dict('agent_0': Box(-inf, inf, (18,), float32))\n",
      "Action size: 9\n",
      "Observation size: 18\n",
      "Environments size: 1\n"
     ]
    }
   ],
   "source": [
    "env = vmas.make_env(\n",
    "    scenario=scenario_name, # Name of the scenario\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=False,  # Use discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    dict_spaces=True, # Use a dictionary for the observation and action spaces, instead of a tuple\n",
    "\n",
    "    # Scenario custom args\n",
    "    enforce_bounds=True, # Enforce boundaries\n",
    "    n_agents=n_agents,\n",
    ")\n",
    "\n",
    "print(\"Action spaces:\", env.action_space)\n",
    "print(\"Observation spaces:\", env.observation_space)\n",
    "\n",
    "# Get list of agents\n",
    "agent_names = [agent.name for agent in env.agents]\n",
    "\n",
    "# Get the name of the first agent\n",
    "first_agent = agent_names[0]\n",
    "\n",
    "# Obtain the size of the action space\n",
    "action_size = env.action_space[first_agent].n\n",
    "\n",
    "# Obtain the size of the observation space\n",
    "observation_size = env.observation_space[first_agent].shape[0]\n",
    "\n",
    "print(\"Action size:\", action_size)\n",
    "print(\"Observation size:\", observation_size)\n",
    "print(\"Environments size:\", num_vmas_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, lr, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        logits = self.actor(obs)\n",
    "\n",
    "        # Apply softmax to get the policy\n",
    "        policy = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Clamp policy probabilities to prevent NaNs and negative probabilities\n",
    "        policy = torch.clamp(policy, min=1e-8, max=1.0)\n",
    "\n",
    "        value = self.critic(obs)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, hidden_size=128):\n",
    "        super(ICM, self).__init__()\n",
    "        # Inverse Model: Predict action from state and next state\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(2 * observation_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        # Forward Model: Predict next state from state and action\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(observation_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, observation_size)\n",
    "        )\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, state, next_state, action):\n",
    "        # Predict action (inverse)\n",
    "        inverse_input = torch.cat([state, next_state], dim=1)\n",
    "        predicted_action = self.inverse_model(inverse_input)\n",
    "\n",
    "        # Predict next state (forward)\n",
    "        forward_input = torch.cat([state, action], dim=1)\n",
    "        predicted_next_state = self.forward_model(forward_input)\n",
    "\n",
    "        return predicted_action, predicted_next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, env, observation_size, action_size):\n",
    "        self.env = env\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Get list of agents names\n",
    "        self.agents = [agent.name for agent in env.agents]\n",
    "\n",
    "        # Entropy-Based Exploration Hyperparameters\n",
    "        self.entropy_coeff = 0.05 # Entropy coefficient\n",
    "\n",
    "        # PPO Hyperparameters\n",
    "        self.gamma = 0.995 # Discount factor\n",
    "        self.lmbda = 0.8 # Lambda for GAE\n",
    "        self.lr = 0.001 # Learning rate\n",
    "        self.clip_epsilon = 0.2 # Clipping epsilon\n",
    "\n",
    "        # Initialize the shared model\n",
    "        self.shared_model = ActorCritic(observation_size, action_size, self.lr, hidden_dim=64).to(device)\n",
    "\n",
    "        # Initialize the curiosity model\n",
    "        self.icm = ICM(observation_size, action_size, hidden_size=128).to(device)\n",
    "        self.intrinsic_reward_weight = 0.001\n",
    "        \n",
    "\n",
    "    def collect_rollout(self, env):\n",
    "        \"\"\"\n",
    "        Runs the environment by a number of steps and returns the collected rollouts\n",
    "        \"\"\"\n",
    "        rollouts = {agent.name: [] for agent in env.agents}\n",
    "        obs = env.reset()\n",
    "        dones = [False]\n",
    "\n",
    "        total_reward = []\n",
    "        avg_reward = 0\n",
    "\n",
    "        # Keep going until done\n",
    "        while True:\n",
    "            actions = {}\n",
    "\n",
    "            # Get the action of each agent\n",
    "            for agent in env.agents:\n",
    "                policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                # Ensure that the policy is normalized\n",
    "                #policy = torch.softmax(policy, dim=-1)\n",
    "\n",
    "                # Obtain the action from the policy\n",
    "                action = torch.multinomial(policy, num_samples=1)\n",
    "                actions[agent.name] = action\n",
    "\n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            for agent in env.agents:\n",
    "                # Convert action to tensor with one-hot encoding\n",
    "                action_tensor = nn.functional.one_hot(actions[agent.name], num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "                # Compute the intrinsic reward\n",
    "                _, predicted_next_state = self.icm(obs[agent.name], next_obs[agent.name], action_tensor)\n",
    "                intrinsic_reward = self.icm.mse_loss(predicted_next_state, next_obs[agent.name])\n",
    "\n",
    "                # Compute the extrinsic reward\n",
    "                extrinsic_reward = rewards[agent.name]\n",
    "\n",
    "                # Combine the intrinsic and extrinsic rewards\n",
    "                comb_rew = extrinsic_reward + intrinsic_reward * self.intrinsic_reward_weight\n",
    "\n",
    "                # Update the average reward\n",
    "                avg_reward = (avg_reward * len(total_reward) + comb_rew) / (len(total_reward) + 1)\n",
    "                total_reward.append(comb_rew)\n",
    "\n",
    "                # Store the rollouts\n",
    "                rollouts[agent.name].append((obs[agent.name], actions[agent.name], comb_rew, next_obs[agent.name], dones))\n",
    "\n",
    "            obs = next_obs\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        return rollouts, avg_reward\n",
    "    \n",
    "    def train_icm(self, rollouts):\n",
    "        \"\"\"\n",
    "        Trains the curiosity model using the rollouts previously collected\n",
    "        \"\"\"\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, actions, _, next_obs, _ = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs = torch.tensor(np.array(obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            next_obs = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            \n",
    "            actions = nn.functional.one_hot(torch.tensor(actions), num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "\n",
    "            # Train the curiosity model\n",
    "            predicted_actions, predicted_next_state = self.icm(obs, next_obs, actions)\n",
    "\n",
    "            # Inverse loss\n",
    "            inverse_loss = nn.CrossEntropyLoss()(predicted_actions, actions)\n",
    "\n",
    "            # Forward loss\n",
    "            forward_loss = self.icm.mse_loss(predicted_next_state, next_obs)\n",
    "\n",
    "            # Total loss\n",
    "            loss = inverse_loss + forward_loss * 0.2\n",
    "\n",
    "            # Backpropagate\n",
    "            self.icm.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.icm.optimizer.step()\n",
    "\n",
    "\n",
    "    def get_advantages(self, rollouts):\n",
    "        \"\"\"\n",
    "        Computes the advantages for each agent using Generalized Advantage Estimation from the rollouts previously collected\n",
    "        \"\"\"\n",
    "        advantages = {agent: [] for agent in rollouts.keys()}\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, _, rewards, next_obs, dones = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs_tensor = torch.tensor(np.array(obs), dtype=torch.float32, device=device)\n",
    "            next_obs_tensor = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device)\n",
    "            dones_tensor = torch.tensor(np.array(dones), dtype=torch.float32, device=device)\n",
    "\n",
    "            # Get the values from the critic\n",
    "            _, values = self.shared_model(obs_tensor)\n",
    "            _, next_values = self.shared_model(next_obs_tensor)\n",
    "\n",
    "            # Remove extra dimensions\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "\n",
    "            # Compute the advantages\n",
    "            returns = []\n",
    "            advantage = 0.0\n",
    "            for reward, value, next_value, done in zip(reversed(rewards), reversed(values), reversed(next_values), reversed(dones_tensor)):\n",
    "                # GAE\n",
    "                td_error = reward + (1 - done) * self.gamma * next_value - value\n",
    "                advantage = td_error + self.gamma * self.lmbda * advantage\n",
    "                returns.insert(0, advantage + value)  # GAE + baseline\n",
    "\n",
    "            advantages[agent] = torch.tensor(returns, dtype=torch.float32, device=device) - values\n",
    "        return advantages\n",
    "\n",
    "    def ppo_update(self, rollouts, advantages, num_iters=10):\n",
    "        all_obs, all_actions, all_adv = [], [], []\n",
    "\n",
    "        # Collect all observations, actions and returns\n",
    "        for agent in rollouts.keys():\n",
    "            obs, actions, _, _, _ = zip(*rollouts[agent])\n",
    "            all_obs.extend(obs)\n",
    "            all_actions.extend(actions)\n",
    "            all_adv.extend(advantages[agent])\n",
    "\n",
    "        # Convert to tensors\n",
    "        obs = torch.tensor(np.array(all_obs), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(all_actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "        adv = torch.tensor(all_adv, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Train the ICM module\n",
    "        self.train_icm(rollouts)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        for _ in range(num_iters):\n",
    "            # Forward pass the Network\n",
    "                policy, values = self.shared_model(obs)\n",
    "                old_policy = policy.detach()\n",
    "                policy = torch.softmax(policy, dim=-1) # Normalize policy\n",
    "\n",
    "                # Remove extra dimension from policy to match actions\n",
    "                policy = policy.squeeze(1)\n",
    "                old_policy = old_policy.squeeze(1)\n",
    "\n",
    "                # Compute action probabilities for policy and old policy\n",
    "                policy_probs = policy.gather(1, actions)  # Select action probabilities\n",
    "                old_policy_probs = old_policy.gather(1, actions)  # Select old action probabilities\n",
    "\n",
    "                # Compute the ratio of probabilities\n",
    "                ratios = (policy_probs / old_policy_probs).squeeze()  # Remove extra dimension\n",
    "\n",
    "                # Policy Loss\n",
    "                clip = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "                policy_loss = -torch.min(ratios * adv, clip * adv).mean()\n",
    "\n",
    "                # Value loss\n",
    "                returns = adv + values.squeeze().detach()\n",
    "                value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                #value_loss = nn.MSELoss()(values.squeeze(), adv + values.squeeze())\n",
    "\n",
    "                # Entropy loss\n",
    "                entropy_loss = -(policy * torch.log(policy + 1e-8)).sum(dim=-1).mean()\n",
    "                self.entropy_coeff = max(self.entropy_coeff * 0.995, 0.01)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + 0.5 * value_loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.shared_model.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.shared_model.parameters(), max_norm=1.0)\n",
    "\n",
    "                self.shared_model.optimizer.step()\n",
    "\n",
    "    def plot_rewards(self, rewards_history):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.title(\"Rewards\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.plot(rewards_history)\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Trains the agents using Proximal Policy Optimization\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            rollouts, avg_reward = self.collect_rollout(self.env)\n",
    "            advantages = self.get_advantages(rollouts)\n",
    "            self.ppo_update(rollouts, advantages)\n",
    "\n",
    "            print(f\"Episode {episode + 1} with average reward of {avg_reward.item():.4f}\", end=\"\\r\", flush=True)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the trained agents using the environment and renders the environment\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            dones = [False]\n",
    "            while not dones[0]:\n",
    "                actions = {}\n",
    "                # Get the action of each agent\n",
    "                for agent in env.agents:\n",
    "                    policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                    # Obtain the action from the policy\n",
    "                    action = torch.multinomial(policy, num_samples=1)\n",
    "                    actions[agent.name] = action\n",
    "                \n",
    "                obs, _, dones, _ = self.env.step(actions)\n",
    "\n",
    "                self.env.render(mode=\"human\")\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs the interactive environment\n",
    "        \"\"\"\n",
    "        vmas.render_interactively(scenario_name, display_info=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 128 with average reward of -0.0000\r"
     ]
    }
   ],
   "source": [
    "ppo = PPO(env, observation_size, action_size)\n",
    "ppo.train(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: TypeError: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 253\u001b[0m, in \u001b[0;36mPPO.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     actions[agent\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    251\u001b[0m obs, _, dones, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/vmas/simulator/environment/environment.py:792\u001b[0m, in \u001b[0;36mEnvironment.render\u001b[0;34m(self, mode, env_index, agent_index_focus, visualize_when_rgb, plot_position_function, plot_position_function_precision, plot_position_function_range, plot_position_function_cmap_range, plot_position_function_cmap_alpha, plot_position_function_cmap_name)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39madd_onetime_list(entity\u001b[38;5;241m.\u001b[39mrender(env_index\u001b[38;5;241m=\u001b[39menv_index))\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# render to display or array\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/vmas/simulator/rendering.py:149\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m    153\u001b[0m text_lines \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementEnv/lib/python3.11/site-packages/pyglet/window/xlib/__init__.py:939\u001b[0m, in \u001b[0;36mXlibWindow.dispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m _view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Check for the events specific to this window\u001b[39;00m\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mxlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXCheckWindowEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x_display\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x1ffffff\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# Key events are filtered by the xlib window event\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# handler so they get a shot at the prefiltered event.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mxany\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (xlib\u001b[38;5;241m.\u001b[39mKeyPress, xlib\u001b[38;5;241m.\u001b[39mKeyRelease):\n\u001b[1;32m    943\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xlib\u001b[38;5;241m.\u001b[39mXFilterEvent(e, \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: TypeError: wrong type"
     ]
    }
   ],
   "source": [
    "ppo.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
