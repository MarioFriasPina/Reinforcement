{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import imageio\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 1024  # Total number of training epochs, i.e. number of steps\n",
    "max_steps = 256  # Episode steps before reset\n",
    "num_vmas_envs = 1  # Number of vectorized environments to simulate at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigation_args = {\n",
    "    \"scenario\" : \"navigation\",\n",
    "    \"n_agents\": 4,\n",
    "    \"shared_rew\": False,\n",
    "}\n",
    "\n",
    "flocking_args = {\n",
    "    \"scenario\" : \"flocking\",\n",
    "    \"n_agents\": 2,\n",
    "}\n",
    "\n",
    "sampling_args = {\n",
    "    \"scenario\" : \"sampling\",\n",
    "    \"n_agents\": 2,\n",
    "    \"shared_rew\": False,\n",
    "}\n",
    "\n",
    "# Choose the scenario\n",
    "args = navigation_args\n",
    "\n",
    "env = vmas.make_env(\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=False, # Use discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=device,\n",
    "    dict_spaces=True, # Use a dictionary for the observation and action spaces, instead of a tuple\n",
    "\n",
    "    # Scenario custom args\n",
    "    **args\n",
    ")\n",
    "\n",
    "print(\"Action spaces:\", env.action_space)\n",
    "print(\"Observation spaces:\", env.observation_space)\n",
    "\n",
    "# Get list of agents\n",
    "agent_names = [agent.name for agent in env.agents]\n",
    "\n",
    "# Get the name of the first agent\n",
    "first_agent = agent_names[0]\n",
    "\n",
    "# Obtain the size of the action space\n",
    "action_size = env.action_space[first_agent].n\n",
    "\n",
    "# Obtain the size of the observation space\n",
    "observation_size = env.observation_space[first_agent].shape[0]\n",
    "\n",
    "print(\"Action size:\", action_size)\n",
    "print(\"Observation size:\", observation_size)\n",
    "print(\"Environments size:\", num_vmas_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, lr=1e-4, hidden_layers=[128]):\n",
    "        \"\"\"\n",
    "        Initialize the actor-critic network.\n",
    "\n",
    "        Args:\n",
    "            observation_size (int): Size of the observation space.\n",
    "            action_size (int): Size of the action space.\n",
    "            lr (float, optional): Learning rate. Defaults to 1e-4.\n",
    "            hidden_layers (list, optional): List of hidden layer sizes. Defaults to [128].\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # The actor and critic share part of the network\n",
    "        layers = [nn.Linear(observation_size, hidden_layers[0]), nn.ReLU()]\n",
    "        for hidden_size in hidden_layers[1:]:\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "\n",
    "        # Actor head, returns a probability distribution over actions\n",
    "        self.actor = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "        # Critic head, returns a value estimate for a given state\n",
    "        self.critic = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Runs the Neural Network forward pass.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            policy (torch.Tensor): The policy distribution over actions.\n",
    "            value (torch.Tensor): The value estimate for the given state.\n",
    "        \"\"\"\n",
    "        x = self.shared(state)\n",
    "\n",
    "        # Apply softmax to get the policy\n",
    "        policy = torch.softmax(self.actor(x), dim=-1)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, lr=1e-4,  hidden_size=128):\n",
    "        super(ICM, self).__init__()\n",
    "        # Inverse Model: Predict action from state and next state\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(2 * observation_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        # Forward Model: Predict next state from state and action\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(observation_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, observation_size)\n",
    "        )\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, state, next_state, action):\n",
    "        # Predict action (inverse)\n",
    "        inverse_input = torch.cat([state, next_state], dim=1)\n",
    "        predicted_action = self.inverse_model(inverse_input)\n",
    "\n",
    "        # Predict next state (forward)\n",
    "        forward_input = torch.cat([state, action], dim=1)\n",
    "        predicted_next_state = self.forward_model(forward_input)\n",
    "\n",
    "        return predicted_action, predicted_next_state\n",
    "    \n",
    "class EMANormalizer:\n",
    "    def __init__(self, alpha=0.99, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average Normalizer, used for normalizing rewards while giving more weights to recent rewards.`\n",
    "        \"\"\"\n",
    "        self.mean = 0.0\n",
    "        self.var = 0.0\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, reward):\n",
    "        delta = reward - self.mean\n",
    "        self.mean += self.alpha * delta\n",
    "        self.var += self.alpha * (delta**2 - self.var)\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        std = (self.var)**0.5 + self.epsilon\n",
    "        return (reward - self.mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, observation_size, action_size, num_epochs=1024):\n",
    "        \"\"\"\n",
    "        Initialize the PPO agent.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to interact with.\n",
    "            observation_size (int): Size of the observation space.\n",
    "            action_size (int): Size of the action space.\n",
    "            num_epochs (int, optional): Number of training epochs. Defaults to 1024.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = len(self.env.agents)\n",
    "\n",
    "        self.training_epochs = num_epochs\n",
    "\n",
    "        # Reward Normalization Functions\n",
    "        self.extrinsic_rn = EMANormalizer()\n",
    "        self.intrinsic_rn = EMANormalizer()\n",
    "\n",
    "        # Early stopping hyperparameters\n",
    "        self.init_patience = 8 # Number of epochs to wait before early stopping\n",
    "        self.patience = self.init_patience\n",
    "        self.min_steps = 64 # Minimum number of epochs before early stopping\n",
    "\n",
    "        # Entropy-Based Exploration Hyperparameters\n",
    "        self.entropy_coeff = 0.02\n",
    "        self.min_entropy_coeff = 0.01\n",
    "        self.entropy_decay = 0.999\n",
    "\n",
    "        # PPO Hyperparameters\n",
    "        self.gamma = 0.99 # Discount factor\n",
    "        self.lmbda = 0.95 # Lambda for GAE\n",
    "        self.clip_epsilon = 0.2 # Clipping epsilon\n",
    "        self.ppo_iters = 8 # Number of PPO iterations\n",
    "\n",
    "        # Learning rates\n",
    "        policy_lr = 1e-3\n",
    "        icm_lr = 1e-3\n",
    "\n",
    "        # Initialize the shared model\n",
    "        layers = [128]\n",
    "        self.shared_model = ActorCritic(observation_size, action_size, lr=policy_lr, hidden_layers=layers).to(device)\n",
    "\n",
    "        # Initialize the curiosity model\n",
    "        self.icm = ICM(observation_size, action_size, lr=icm_lr, hidden_size=128).to(device)\n",
    "        self.intrinsic_reward_weight = 1.0\n",
    "        self.extrinsic_reward_weight = 1.0\n",
    "        self.min_intrinsic_reward = 0.001\n",
    "        self.decay_factor = 0.99\n",
    "\n",
    "    def collect_rollout(self, env):\n",
    "        \"\"\"\n",
    "        Runs the environment by a number of steps and returns the collected rollouts\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to interact with.\n",
    "\n",
    "        Returns:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "            steps (int): The number of steps taken in the environment for this episode.\n",
    "            rewards (dict): A dictionary containing the total rewards for each agent.\n",
    "        \"\"\"\n",
    "        rollouts = {agent.name: [] for agent in env.agents}\n",
    "        list_rewards = []\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        dones = [False]\n",
    "\n",
    "        # Keep going until done\n",
    "        while True:\n",
    "            steps += 1\n",
    "            actions = {}\n",
    "\n",
    "            # Get the action of each agent\n",
    "            for agent in env.agents:\n",
    "                policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                # Obtain the action from the policy\n",
    "                action = torch.multinomial(policy, num_samples=1)\n",
    "                actions[agent.name] = action\n",
    "\n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            for agent in env.agents:\n",
    "                # Convert action to tensor with one-hot encoding\n",
    "                action_tensor = nn.functional.one_hot(actions[agent.name], num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "                # Compute the intrinsic reward\n",
    "                _, predicted_next_state = self.icm(obs[agent.name], next_obs[agent.name], action_tensor)\n",
    "                intrinsic_reward = self.icm.mse_loss(predicted_next_state, next_obs[agent.name])\n",
    "\n",
    "                extrinsic_reward = rewards[agent.name]\n",
    "\n",
    "                # Update the reward normalizers\n",
    "                self.extrinsic_rn.update(extrinsic_reward)\n",
    "                self.intrinsic_rn.update(intrinsic_reward)\n",
    "\n",
    "                # Normalize both rewards\n",
    "                extrinsic_reward = self.extrinsic_rn.normalize(extrinsic_reward)\n",
    "                intrinsic_reward = self.intrinsic_rn.normalize(intrinsic_reward)\n",
    "\n",
    "                # Combine the intrinsic and extrinsic rewards using the hyperparameters\n",
    "                comb_rew = extrinsic_reward * self.extrinsic_reward_weight + intrinsic_reward * self.intrinsic_reward_weight\n",
    "\n",
    "                list_rewards.append(extrinsic_reward)\n",
    "\n",
    "                # Store the rollouts\n",
    "                rollouts[agent.name].append((obs[agent.name], actions[agent.name], comb_rew.item(), next_obs[agent.name], dones))\n",
    "\n",
    "            obs = next_obs\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        avg_reward = sum(list_rewards) / len(list_rewards)\n",
    "\n",
    "        return rollouts, steps, avg_reward.item()\n",
    "    \n",
    "    def train_icm(self, rollouts):\n",
    "        \"\"\"\n",
    "        Trains the curiosity model using the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "        \"\"\"\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, actions, _, next_obs, _ = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs = torch.stack(obs).squeeze(1)\n",
    "            next_obs = torch.stack(next_obs).squeeze(1)\n",
    "            \n",
    "            actions = nn.functional.one_hot(torch.tensor(actions), num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "\n",
    "            # Train the curiosity model\n",
    "            predicted_actions, predicted_next_state = self.icm(obs, next_obs, actions)\n",
    "\n",
    "            # Inverse loss\n",
    "            inverse_loss = nn.CrossEntropyLoss()(predicted_actions, actions)\n",
    "\n",
    "            # Forward loss\n",
    "            forward_loss = self.icm.mse_loss(predicted_next_state, next_obs)\n",
    "\n",
    "            # Total loss\n",
    "            loss = inverse_loss + forward_loss * 0.2\n",
    "\n",
    "            # Backpropagate\n",
    "            self.icm.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.icm.optimizer.step()\n",
    "\n",
    "    def get_advantages(self, rollouts):\n",
    "        \"\"\"\n",
    "        Computes the advantages for each agent using Generalized Advantage Estimation from the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "\n",
    "        Returns:\n",
    "            advantages (dict): A dictionary containing the advantages for each agent.\n",
    "        \"\"\"\n",
    "        advantages = {agent: [] for agent in rollouts.keys()}\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, _, rewards, next_obs, dones = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert tuples of tensors to tensors\n",
    "            obs = torch.stack(obs)\n",
    "            next_obs = torch.stack(next_obs)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Get the values from the critic\n",
    "            _, values = self.shared_model(obs)\n",
    "            _, next_values = self.shared_model(next_obs)\n",
    "\n",
    "            # Remove extra dimensions\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "\n",
    "            # Compute the advantages\n",
    "            returns = []\n",
    "            advantage = 0.0\n",
    "            for reward, value, next_value, done in zip(reversed(rewards), reversed(values), reversed(next_values), reversed(dones)):\n",
    "                # Generalized Advantage Estimation\n",
    "                td_error = reward + (1 - done) * self.gamma * next_value - value\n",
    "                advantage = td_error + self.gamma * self.lmbda * advantage\n",
    "                returns.insert(0, advantage + value)  # GAE + baseline\n",
    "\n",
    "            advantages[agent] = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def ppo_update(self, rollouts, advantages):\n",
    "        \"\"\"\n",
    "        Updates the policy using Proximal Policy Optimization from the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "            advantages (dict): A dictionary containing the advantages for each agent.\n",
    "\n",
    "        Returns:\n",
    "            policy_loss (float): The loss of the policy.\n",
    "        \"\"\"\n",
    "        all_obs, all_adv, all_actions = [], [], []\n",
    "\n",
    "        # Collect all observations, actions and returns of the different agents into a single tensor\n",
    "        for agent in rollouts.keys():\n",
    "            obs, act, _, _, _ = zip(*rollouts[agent])\n",
    "            all_obs.extend(obs)\n",
    "            all_adv.extend(advantages[agent])\n",
    "            all_actions.extend(act)\n",
    "\n",
    "        # Stack into a single tensor\n",
    "        obs = torch.stack(all_obs)\n",
    "        act = torch.stack(all_actions)\n",
    "        adv = torch.stack(all_adv)\n",
    "\n",
    "        # Train the ICM module\n",
    "        self.train_icm(rollouts)\n",
    "\n",
    "        policy, _ = self.shared_model(obs)\n",
    "\n",
    "        # Get the shape of the policy and the actions to be compatible\n",
    "        policy = policy.squeeze(1)\n",
    "        act = act.squeeze(1)\n",
    "\n",
    "        old_policy_probs = policy.detach().gather(1, act)\n",
    "\n",
    "        for _ in range(self.ppo_iters):\n",
    "            policy, values = self.shared_model(obs)\n",
    "\n",
    "            policy = policy.squeeze(1)\n",
    "            # Compute action probabilities\n",
    "            policy_probs = policy.gather(1, act)\n",
    "\n",
    "            # Compute the ratio of probabilities\n",
    "            ratios = (policy_probs / old_policy_probs).squeeze()\n",
    "            clip = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "\n",
    "            # Compute returns, advantages minus baseline\n",
    "            returns = adv - values.squeeze().detach()\n",
    "\n",
    "            # Policy loss\n",
    "            policy_loss = -torch.min(ratios * returns, clip * returns).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), adv)\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = -(policy * torch.log(policy + 1e-8)).sum(dim=-1).mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "            # Backward pass\n",
    "            self.shared_model.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            #torch.nn.utils.clip_grad_norm_(self.shared_model.parameters(), max_norm=1.0)\n",
    "\n",
    "            self.shared_model.optimizer.step()           \n",
    "\n",
    "        return policy_loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agents using Proximal Policy Optimization\n",
    "        \"\"\"\n",
    "        def update_plot(fig, ax1, ax2, df, hdisplay):\n",
    "            # Update the plot title\n",
    "            fig.suptitle(f'{args[\"scenario\"]}: Episode {df[\"episode\"].iloc[-1]}')\n",
    "\n",
    "            ax1.clear()\n",
    "            ax2.clear()\n",
    "            ax3.clear()\n",
    "\n",
    "            ax1.plot(df['episode'], df['steps'], label='Steps')\n",
    "            ax1.plot(df['episode'], df['mean_steps'], label='Mean Steps')\n",
    "            ax1.set_title('Steps per Episode:')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Steps')\n",
    "            ax1.legend()\n",
    "\n",
    "            ax2.plot(df['episode'], df['loss'], label='Loss')\n",
    "            ax2.plot(df['episode'], df['mean_loss'], label='Mean Loss')\n",
    "            ax2.set_title('Loss per Episode')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Loss')\n",
    "            ax2.legend()\n",
    "\n",
    "            ax3.plot(df['episode'], df['reward'], label='Reward')\n",
    "            ax3.plot(df['episode'], df['mean_reward'], label='Mean Reward')\n",
    "            ax3.set_title('Reward per Episode')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Reward')\n",
    "            ax3.legend()\n",
    "\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            hdisplay.update(fig)\n",
    "\n",
    "        \"\"\" Training Logging \"\"\"\n",
    "        df = pd.DataFrame(columns=['episode', 'steps', 'mean_steps', 'loss', 'mean_loss', 'reward', 'mean_reward'])\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        hdisplay = display.display(\"Starting training\", display_id=True)\n",
    "\n",
    "        best = float('inf')\n",
    "\n",
    "        for episode in range(self.training_epochs):\n",
    "            \"\"\" PPO Rollout and Update \"\"\"\n",
    "            rollouts, steps, reward = self.collect_rollout(self.env)\n",
    "            advantages = self.get_advantages(rollouts)\n",
    "            policy_loss = self.ppo_update(rollouts, advantages)\n",
    "\n",
    "            \"\"\" Hyperparameters Update \"\"\"\n",
    "            # Update the entropy coefficient, to make the policy less explorative as it improves\n",
    "            self.entropy_coeff = max(self.entropy_coeff * self.entropy_decay, self.min_entropy_coeff)\n",
    "            # Update the instrinsic reward scale, to make the extrinsic reward more important over time\n",
    "            self.intrinsic_reward_weight = max(self.intrinsic_reward_weight * self.decay_factor, self.min_intrinsic_reward)\n",
    "\n",
    "            \"\"\" Logging \"\"\"\n",
    "            # Calculate the mean of the last min_steps\n",
    "            mean_steps = df['steps'].iloc[-self.min_steps:].mean()\n",
    "            mean_loss = df['loss'].iloc[-self.min_steps:].mean()\n",
    "            mean_reward = df['reward'].iloc[-self.min_steps:].mean()\n",
    "\n",
    "            df.loc[episode] = [episode, steps, mean_steps, policy_loss, mean_loss, reward, mean_reward]\n",
    "            update_plot(fig, ax1, ax2, df, hdisplay)\n",
    "\n",
    "            \"\"\" Early Stopping \"\"\"\n",
    "            # Reset patience as the model is improving\n",
    "            if episode >= best:\n",
    "                if df['mean_steps'].iloc[-1] <= best:\n",
    "                    self.patience = self.max_patience\n",
    "                    best = df['mean_steps'].iloc[-1]\n",
    "                else:\n",
    "                    self.patience -= 1\n",
    "                    if self.patience == 0:\n",
    "                        break\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the trained agents using the environment and renders the environment\n",
    "        \"\"\"\n",
    "\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            dones = [False]\n",
    "            while not dones[0]:\n",
    "                actions = {}\n",
    "                # Get the action of each agent\n",
    "                for agent in env.agents:\n",
    "                    policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                    # Obtain the action from the policy\n",
    "                    action = torch.multinomial(policy, num_samples=1)\n",
    "                    actions[agent.name] = action\n",
    "                \n",
    "                obs, _, dones, _ = self.env.step(actions)\n",
    "\n",
    "                self.env.render(mode=\"human\")\n",
    "    \n",
    "    def create_gif(self, path, num_simulations=1):\n",
    "        \"\"\"\n",
    "        Creates a gif of the environment\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            obs = self.env.reset()\n",
    "            dones = [False]\n",
    "            while not dones[0]:\n",
    "                frame = self.env.render(mode=\"rgb_array\")\n",
    "                frames.append(frame)\n",
    "\n",
    "                actions = {}\n",
    "                # Get the action of each agent\n",
    "                for agent in env.agents:\n",
    "                    policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                    # Obtain the action from the policy\n",
    "                    action = torch.multinomial(policy, num_samples=1)\n",
    "                    actions[agent.name] = action\n",
    "                \n",
    "                obs, _, dones, _ = self.env.step(actions)\n",
    "\n",
    "        imageio.mimsave(path, frames, 'GIF', fps=30)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.shared_model.state_dict(), f\"models/{args['scenario']}_policy.pth\")\n",
    "        torch.save(self.icm.state_dict(), f\"models/{args['scenario']}_icm.pth\")\n",
    "\n",
    "    def load(self):\n",
    "        self.shared_model.load_state_dict(torch.load(f\"models/{args['scenario']}_policy.pth\"))\n",
    "        self.icm.load_state_dict(torch.load(f\"models/{args['scenario']}_icm.pth\"))\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs the interactive environment\n",
    "        \"\"\"\n",
    "        vmas.render_interactively(args.scenario, display_info=False)\n",
    "\n",
    "ppo = PPO(env, observation_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ppo.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gif of the environment\n",
    "ppo.create_gif(f\"{args['scenario']}.gif\", num_simulations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install vmas\n",
    "!pip install vmas[all]\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install imageio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
