{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "is_fork = multiprocessing.get_start_method() == 'fork'\n",
    "device = (\n",
    "    torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device('cpu')\n",
    ")\n",
    "vmas_device = device\n",
    "\n",
    "num_epochs = 1024  # Total number of training epochs, i.e. number of steps\n",
    "max_steps = 256  # Episode steps before reset\n",
    "num_vmas_envs = 1  # Number of vectorized environments to simulate at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 2 # Number of agents in the environment\n",
    "\n",
    "navigation_args = {\n",
    "    \"scenario\" : \"navigation\",\n",
    "    \"n_agents\": n_agents,\n",
    "    \"shared_rew\": False,\n",
    "}\n",
    "\n",
    "discovery_args = {\n",
    "    \"scenario\" : \"discovery\",\n",
    "    \"n_agents\": n_agents,\n",
    "    \"n_targets\": n_agents,\n",
    "    \"agents_per_target\": n_agents // 2,\n",
    "}\n",
    "\n",
    "# Choose the scenario\n",
    "args = navigation_args\n",
    "#args = discovery_args\n",
    "\n",
    "env = vmas.make_env(\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=False,\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    dict_spaces=True, # Use a dictionary for the observation and action spaces, instead of a tuple\n",
    "\n",
    "    # Scenario custom args\n",
    "    **args\n",
    ")\n",
    "\n",
    "print(\"Action spaces:\", env.action_space)\n",
    "print(\"Observation spaces:\", env.observation_space)\n",
    "\n",
    "# Get list of agents\n",
    "agent_names = [agent.name for agent in env.agents]\n",
    "\n",
    "# Get the name of the first agent\n",
    "first_agent = agent_names[0]\n",
    "\n",
    "# Obtain the size of the action space\n",
    "action_size = env.action_space[first_agent].n\n",
    "\n",
    "# Obtain the size of the observation space\n",
    "observation_size = env.observation_space[first_agent].shape[0]\n",
    "\n",
    "print(\"Action size:\", action_size)\n",
    "print(\"Observation size:\", observation_size)\n",
    "print(\"Environments size:\", num_vmas_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, lr=1e-4, hidden_layers=[128]):\n",
    "        \"\"\"\n",
    "        Initialize the actor-critic network.\n",
    "\n",
    "        Args:\n",
    "            observation_size (int): Size of the observation space.\n",
    "            action_size (int): Size of the action space.\n",
    "            lr (float, optional): Learning rate. Defaults to 1e-4.\n",
    "            hidden_layers (list, optional): List of hidden layer sizes. Defaults to [128].\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # The actor and critic share part of the network\n",
    "        layers = [nn.Linear(observation_size, hidden_layers[0]), nn.ReLU()]\n",
    "        for hidden_size in hidden_layers[1:]:\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "\n",
    "        # Actor head, returns a probability distribution over actions\n",
    "        self.actor = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "        # Critic head, returns a value estimate for a given state\n",
    "        self.critic = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Runs the Neural Network forward pass.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            policy (torch.Tensor): The policy distribution over actions.\n",
    "            value (torch.Tensor): The value estimate for the given state.\n",
    "        \"\"\"\n",
    "        x = self.shared(state)\n",
    "\n",
    "        # Apply softmax to get the policy\n",
    "        policy = torch.softmax(self.actor(x), dim=-1)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, lr=1e-4,  hidden_size=128):\n",
    "        super(ICM, self).__init__()\n",
    "        # Inverse Model: Predict action from state and next state\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(2 * observation_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        # Forward Model: Predict next state from state and action\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(observation_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, observation_size)\n",
    "        )\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, state, next_state, action):\n",
    "        # Predict action (inverse)\n",
    "        inverse_input = torch.cat([state, next_state], dim=1)\n",
    "        predicted_action = self.inverse_model(inverse_input)\n",
    "\n",
    "        # Predict next state (forward)\n",
    "        forward_input = torch.cat([state, action], dim=1)\n",
    "        predicted_next_state = self.forward_model(forward_input)\n",
    "\n",
    "        return predicted_action, predicted_next_state\n",
    "    \n",
    "class EMANormalizer:\n",
    "    def __init__(self, alpha=0.99, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 0.0\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, reward):\n",
    "        delta = reward - self.mean\n",
    "        self.mean += self.alpha * delta\n",
    "        self.var += self.alpha * (delta**2 - self.var)\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        std = (self.var)**0.5 + self.epsilon\n",
    "        return (reward - self.mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, observation_size, action_size, num_epochs=1024):\n",
    "        \"\"\"\n",
    "        Initialize the PPO agent.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to interact with.\n",
    "            observation_size (int): Size of the observation space.\n",
    "            action_size (int): Size of the action space.\n",
    "            num_epochs (int, optional): Number of training epochs. Defaults to 1024.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = len(self.env.agents)\n",
    "\n",
    "        self.training_epochs = num_epochs\n",
    "\n",
    "        # Reward Normalization Functions\n",
    "        self.extrinsic_rn = EMANormalizer()\n",
    "        self.intrinsic_rn = EMANormalizer()\n",
    "\n",
    "        # Early stopping hyperparameters\n",
    "        self.init_patience = 8 # Number of epochs to wait before early stopping\n",
    "        self.patience = self.init_patience\n",
    "        self.min_steps = 64 # Minimum number of epochs before early stopping\n",
    "\n",
    "        # Entropy-Based Exploration Hyperparameters\n",
    "        self.entropy_coeff = 0.02\n",
    "        self.min_entropy_coeff = 0.01\n",
    "        self.entropy_decay = 0.999\n",
    "\n",
    "        # PPO Hyperparameters\n",
    "        self.gamma = 0.999 # Discount factor\n",
    "        self.lmbda = 0.8 # Lambda for GAE\n",
    "        self.clip_epsilon = 0.2 # Clipping epsilon\n",
    "        self.ppo_iters = 8 # Number of PPO iterations\n",
    "\n",
    "        # Learning rates\n",
    "        policy_lr = 1e-3\n",
    "        icm_lr = 1e-4\n",
    "\n",
    "        # Initialize the shared model\n",
    "        layers = [128]\n",
    "        self.shared_model = ActorCritic(observation_size, action_size, lr=policy_lr, hidden_layers=layers).to(device)\n",
    "\n",
    "        # Initialize the curiosity model\n",
    "        self.icm = ICM(observation_size, action_size, lr=icm_lr, hidden_size=128).to(device)\n",
    "        self.intrinsic_reward_weight = 1.0\n",
    "        self.extrinsic_reward_weight = 1.0\n",
    "        self.min_intrinsic_reward = 0.001\n",
    "        self.decay_factor = 0.9\n",
    "\n",
    "    def collect_rollout(self, env):\n",
    "        \"\"\"\n",
    "        Runs the environment by a number of steps and returns the collected rollouts\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to interact with.\n",
    "\n",
    "        Returns:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "            steps (int): The number of steps taken in the environment for this episode.\n",
    "            rewards (dict): A dictionary containing the total rewards for each agent.\n",
    "        \"\"\"\n",
    "        rollouts = {agent.name: [] for agent in env.agents}\n",
    "        list_rewards = []\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        dones = [False]\n",
    "\n",
    "        # Keep going until done\n",
    "        while True:\n",
    "            steps += 1\n",
    "            actions = {}\n",
    "\n",
    "            # Get the action of each agent\n",
    "            for agent in env.agents:\n",
    "                policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                # Obtain the action from the policy\n",
    "                action = torch.multinomial(policy, num_samples=1)\n",
    "                actions[agent.name] = action\n",
    "\n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            for agent in env.agents:\n",
    "                # Convert action to tensor with one-hot encoding\n",
    "                action_tensor = nn.functional.one_hot(actions[agent.name], num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "                # Compute the intrinsic reward\n",
    "                _, predicted_next_state = self.icm(obs[agent.name], next_obs[agent.name], action_tensor)\n",
    "                intrinsic_reward = self.icm.mse_loss(predicted_next_state, next_obs[agent.name])\n",
    "\n",
    "                extrinsic_reward = rewards[agent.name]\n",
    "\n",
    "                # Update the reward normalizers\n",
    "                self.extrinsic_rn.update(extrinsic_reward)\n",
    "                self.intrinsic_rn.update(intrinsic_reward)\n",
    "\n",
    "                # Normalize both rewards\n",
    "                extrinsic_reward = self.extrinsic_rn.normalize(extrinsic_reward)\n",
    "                intrinsic_reward = self.intrinsic_rn.normalize(intrinsic_reward)\n",
    "\n",
    "                # Combine the intrinsic and extrinsic rewards using the hyperparameters\n",
    "                comb_rew = extrinsic_reward * self.extrinsic_reward_weight + intrinsic_reward * self.intrinsic_reward_weight\n",
    "\n",
    "                list_rewards.append(extrinsic_reward)\n",
    "\n",
    "                # Store the rollouts\n",
    "                rollouts[agent.name].append((obs[agent.name], actions[agent.name], comb_rew.item(), next_obs[agent.name], dones))\n",
    "\n",
    "            obs = next_obs\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        avg_reward = sum(list_rewards) / len(list_rewards)\n",
    "\n",
    "        return rollouts, steps, avg_reward\n",
    "    \n",
    "    def train_icm(self, rollouts):\n",
    "        \"\"\"\n",
    "        Trains the curiosity model using the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "        \"\"\"\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, actions, _, next_obs, _ = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs = torch.tensor(np.array(obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            next_obs = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            \n",
    "            actions = nn.functional.one_hot(torch.tensor(actions), num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "\n",
    "            # Train the curiosity model\n",
    "            predicted_actions, predicted_next_state = self.icm(obs, next_obs, actions)\n",
    "\n",
    "            # Inverse loss\n",
    "            inverse_loss = nn.CrossEntropyLoss()(predicted_actions, actions)\n",
    "\n",
    "            # Forward loss\n",
    "            forward_loss = self.icm.mse_loss(predicted_next_state, next_obs)\n",
    "\n",
    "            # Total loss\n",
    "            loss = inverse_loss + forward_loss * 0.2\n",
    "\n",
    "            # Backpropagate\n",
    "            self.icm.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.icm.optimizer.step()\n",
    "\n",
    "    def get_advantages(self, rollouts):\n",
    "        \"\"\"\n",
    "        Computes the advantages for each agent using Generalized Advantage Estimation from the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "\n",
    "        Returns:\n",
    "            advantages (dict): A dictionary containing the advantages for each agent.\n",
    "        \"\"\"\n",
    "        advantages = {agent: [] for agent in rollouts.keys()}\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, _, rewards, next_obs, dones = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs_tensor = torch.tensor(np.array(obs), dtype=torch.float32, device=device)\n",
    "            next_obs_tensor = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device)\n",
    "            dones_tensor = torch.tensor(np.array(dones), dtype=torch.float32, device=device)\n",
    "            rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=device)\n",
    "\n",
    "            # Get the values from the critic\n",
    "            _, values = self.shared_model(obs_tensor)\n",
    "            _, next_values = self.shared_model(next_obs_tensor)\n",
    "\n",
    "            # Remove extra dimensions\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "\n",
    "            # Normalize rewards\n",
    "            #rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "            # Show biggest and smallest rewards\n",
    "            #print(f\"Rewards mean: {rewards.mean().item()}, std: {rewards.std().item()}, min: {rewards.min().item()}, max: {rewards.max().item()}\")\n",
    "\n",
    "            # Compute the advantages\n",
    "            returns = []\n",
    "            advantage = 0.0\n",
    "            for reward, value, next_value, done in zip(reversed(rewards), reversed(values), reversed(next_values), reversed(dones_tensor)):\n",
    "                # Generalized Advantage Estimation\n",
    "                td_error = reward + (1 - done) * self.gamma * next_value - value\n",
    "                advantage = td_error + self.gamma * self.lmbda * advantage\n",
    "                returns.insert(0, advantage + value)  # GAE + baseline\n",
    "\n",
    "            advantages[agent] = torch.tensor(returns, dtype=torch.float32, device=device) - values\n",
    "\n",
    "            # Normalize advantages\n",
    "            #advantages[agent] = (advantages[agent] - advantages[agent].mean()) / (advantages[agent].std() + 1e-8)\n",
    "\n",
    "            # Show advantages\n",
    "            #print(f\"Advantages mean: {advantages[agent].mean().item():.4f}, std: {advantages[agent].std().item():.4f}\")\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def ppo_update(self, rollouts, advantages):\n",
    "        \"\"\"\n",
    "        Updates the policy using Proximal Policy Optimization from the rollouts previously collected\n",
    "\n",
    "        Args:\n",
    "            rollouts (dict): A dictionary containing the collected rollouts for each agent.\n",
    "            advantages (dict): A dictionary containing the advantages for each agent.\n",
    "\n",
    "        Returns:\n",
    "            policy_loss (float): The loss of the policy.\n",
    "        \"\"\"\n",
    "        all_obs, all_adv, all_actions = [], [], []\n",
    "\n",
    "        # Collect all observations, actions and returns of the different agents into a single tensor\n",
    "        for agent in rollouts.keys():\n",
    "            obs, act, _, _, _ = zip(*rollouts[agent])\n",
    "            all_obs.extend(obs)\n",
    "            all_adv.extend(advantages[agent])\n",
    "            all_actions.extend(act)\n",
    "\n",
    "        obs = torch.tensor(np.array(all_obs), dtype=torch.float32, device=device)\n",
    "        adv = torch.tensor(all_adv, dtype=torch.float32, device=device)\n",
    "        act = torch.tensor(all_actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "\n",
    "        # Train the ICM module\n",
    "        self.train_icm(rollouts)\n",
    "\n",
    "        policy, _ = self.shared_model(obs)\n",
    "        old_policy_probs = policy.detach().squeeze(1).gather(1, act)\n",
    "\n",
    "        for _ in range(self.ppo_iters):\n",
    "            policy, values = self.shared_model(obs)\n",
    "\n",
    "            policy = policy.squeeze(1)\n",
    "            # Compute action probabilities\n",
    "            policy_probs = policy.gather(1, act)\n",
    "\n",
    "            # Compute the ratio of probabilities\n",
    "            ratios = (policy_probs / old_policy_probs).squeeze()\n",
    "\n",
    "            # Policy Loss\n",
    "            clip = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "            policy_loss = -torch.min(ratios * adv, clip * adv).mean()\n",
    "\n",
    "            # Value loss\n",
    "            returns = adv + values.squeeze().detach()\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = -(policy * torch.log(policy + 1e-8)).sum(dim=-1).mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "            # Backward pass\n",
    "            self.shared_model.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.shared_model.parameters(), max_norm=1.0)\n",
    "\n",
    "            self.shared_model.optimizer.step()           \n",
    "\n",
    "        return policy_loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agents using Proximal Policy Optimization\n",
    "        \"\"\"\n",
    "        def update_plot(fig, ax1, ax2, df, hdisplay):\n",
    "            # Update the plot title\n",
    "            fig.suptitle(f'{args[\"scenario\"]}: Episode {df[\"episode\"].iloc[-1]}')\n",
    "\n",
    "            ax1.clear()\n",
    "            ax2.clear()\n",
    "            ax3.clear()\n",
    "\n",
    "            ax1.plot(df['episode'], df['steps'], label='Steps')\n",
    "            ax1.plot(df['episode'], df['mean_steps'], label='Mean Steps')\n",
    "            ax1.set_title('Steps per Episode:')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Steps')\n",
    "            ax1.legend()\n",
    "\n",
    "            ax2.plot(df['episode'], df['loss'], label='Loss')\n",
    "            ax2.plot(df['episode'], df['mean_loss'], label='Mean Loss')\n",
    "            ax2.set_title('Loss per Episode')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Loss')\n",
    "            ax2.legend()\n",
    "\n",
    "            ax3.plot(df['episode'], df['reward'], label='Reward')\n",
    "            ax3.plot(df['episode'], df['mean_reward'], label='Mean Reward')\n",
    "            ax3.set_title('Reward per Episode')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Reward')\n",
    "            ax3.legend()\n",
    "\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            hdisplay.update(fig)\n",
    "\n",
    "        \"\"\" Training Logging \"\"\"\n",
    "        df = pd.DataFrame(columns=['episode', 'steps', 'mean_steps', 'loss', 'mean_loss', 'reward', 'mean_reward'])\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        hdisplay = display.display(\"Starting training\", display_id=True)\n",
    "\n",
    "        best = float('inf')\n",
    "\n",
    "        for episode in range(self.training_epochs):\n",
    "            \"\"\" PPO Rollout and Update \"\"\"\n",
    "            rollouts, steps, reward = self.collect_rollout(self.env)\n",
    "            advantages = self.get_advantages(rollouts)\n",
    "            policy_loss = self.ppo_update(rollouts, advantages)\n",
    "\n",
    "            \"\"\" Hyperparameters Update \"\"\"\n",
    "            # Update the entropy coefficient, to make the policy less explorative as it improves\n",
    "            self.entropy_coeff = max(self.entropy_coeff * self.entropy_decay, self.min_entropy_coeff)\n",
    "            # Update the instrinsic reward scale, to make the extrinsic reward more important over time\n",
    "            self.intrinsic_reward_weight = max(self.intrinsic_reward_weight * self.decay_factor, self.min_intrinsic_reward)\n",
    "\n",
    "            \"\"\" Logging \"\"\"\n",
    "            df.loc[episode] = [episode, steps, df['steps'].mean(), policy_loss, df['loss'].mean(), reward, df['reward'].mean()]\n",
    "            update_plot(fig, ax1, ax2, df, hdisplay)\n",
    "\n",
    "            \"\"\" Early Stopping \"\"\"\n",
    "            # Reset patience as the model is improving\n",
    "            if episode >= best:\n",
    "                if df['mean_steps'].iloc[-1] >= best:\n",
    "                    self.patience = self.max_patience\n",
    "                    best = df['mean_loss'].iloc[-1]\n",
    "                else:\n",
    "                    self.patience -= 1\n",
    "                    if self.patience == 0:\n",
    "                        break\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the trained agents using the environment and renders the environment\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            dones = [False]\n",
    "            while not dones[0]:\n",
    "                actions = {}\n",
    "                # Get the action of each agent\n",
    "                for agent in env.agents:\n",
    "                    policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                    # Obtain the action from the policy\n",
    "                    action = torch.multinomial(policy, num_samples=1)\n",
    "                    actions[agent.name] = action\n",
    "                \n",
    "                obs, _, dones, _ = self.env.step(actions)\n",
    "\n",
    "                self.env.render(mode=\"human\")\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.shared_model.state_dict(), f\"models/{args['scenario']}_policy.pth\")\n",
    "        torch.save(self.icm.state_dict(), f\"models/{args['scenario']}_icm.pth\")\n",
    "\n",
    "    def load(self):\n",
    "        self.shared_model.load_state_dict(torch.load(f\"models/{args['scenario']}_policy.pth\"))\n",
    "        self.icm.load_state_dict(torch.load(f\"models/{args['scenario']}_icm.pth\"))\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs the interactive environment\n",
    "        \"\"\"\n",
    "        vmas.render_interactively(args.scenario, display_info=False)\n",
    "\n",
    "ppo = PPO(env, observation_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ppo.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vmas\n",
    "!pip install vmas[all]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
