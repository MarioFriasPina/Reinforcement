{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "is_fork = multiprocessing.get_start_method() == 'fork'\n",
    "device = (\n",
    "    torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device('cpu')\n",
    ")\n",
    "vmas_device = device\n",
    "\n",
    "# Environment\n",
    "scenario_name = \"navigation\"\n",
    "\n",
    "num_epochs = 1024  # Total number of training epochs, i.e. number of steps\n",
    "max_steps = 256  # Episode steps before reset\n",
    "num_vmas_envs = 1  # Number of vectorized environments to simulate at once\n",
    "\n",
    "n_agents = 1 # Number of agents in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = vmas.make_env(\n",
    "    scenario=scenario_name, # Name of the scenario\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=False,  # Use discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    dict_spaces=True, # Use a dictionary for the observation and action spaces, instead of a tuple\n",
    "\n",
    "    # Scenario custom args\n",
    "    enforce_bounds=True, # Enforce boundaries\n",
    "    n_agents=n_agents,\n",
    ")\n",
    "\n",
    "print(\"Action spaces:\", env.action_space)\n",
    "print(\"Observation spaces:\", env.observation_space)\n",
    "\n",
    "# Get list of agents\n",
    "agent_names = [agent.name for agent in env.agents]\n",
    "\n",
    "# Get the name of the first agent\n",
    "first_agent = agent_names[0]\n",
    "\n",
    "# Obtain the size of the action space\n",
    "action_size = env.action_space[first_agent].n\n",
    "\n",
    "# Obtain the size of the observation space\n",
    "observation_size = env.observation_space[first_agent].shape[0]\n",
    "\n",
    "print(\"Action size:\", action_size)\n",
    "print(\"Observation size:\", observation_size)\n",
    "print(\"Environments size:\", num_vmas_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, lr, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        logits = self.actor(obs)\n",
    "\n",
    "        # Apply softmax to get the policy\n",
    "        policy = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Clamp policy probabilities to prevent NaNs and negative probabilities\n",
    "        policy = torch.clamp(policy, min=1e-8, max=1.0)\n",
    "\n",
    "        value = self.critic(obs)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, observation_size, action_size, hidden_size=128):\n",
    "        super(ICM, self).__init__()\n",
    "        # Inverse Model: Predict action from state and next state\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(2 * observation_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        # Forward Model: Predict next state from state and action\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(observation_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, observation_size)\n",
    "        )\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, state, next_state, action):\n",
    "        # Predict action (inverse)\n",
    "        inverse_input = torch.cat([state, next_state], dim=1)\n",
    "        predicted_action = self.inverse_model(inverse_input)\n",
    "\n",
    "        # Predict next state (forward)\n",
    "        forward_input = torch.cat([state, action], dim=1)\n",
    "        predicted_next_state = self.forward_model(forward_input)\n",
    "\n",
    "        return predicted_action, predicted_next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, env, observation_size, action_size):\n",
    "        self.env = env\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Get list of agents names\n",
    "        self.agents = [agent.name for agent in env.agents]\n",
    "\n",
    "        # Entropy-Based Exploration Hyperparameters\n",
    "        self.entropy_coeff = 0.02 # Entropy coefficient\n",
    "\n",
    "        # PPO Hyperparameters\n",
    "        self.gamma = 0.999 # Discount factor\n",
    "        self.lmbda = 0.8 # Lambda for GAE\n",
    "        self.lr = 0.0001 # Learning rate\n",
    "        self.clip_epsilon = 0.2 # Clipping epsilon\n",
    "\n",
    "        # Initialize the shared model\n",
    "        self.shared_model = ActorCritic(observation_size, action_size, self.lr, hidden_dim=128).to(device)\n",
    "\n",
    "        # Initialize the curiosity model\n",
    "        self.icm = ICM(observation_size, action_size, hidden_size=128).to(device)\n",
    "        self.intrinsic_reward_weight = 0.001\n",
    "        \n",
    "\n",
    "    def collect_rollout(self, env):\n",
    "        \"\"\"\n",
    "        Runs the environment by a number of steps and returns the collected rollouts\n",
    "        \"\"\"\n",
    "        rollouts = {agent.name: [] for agent in env.agents}\n",
    "        obs = env.reset()\n",
    "        dones = [False]\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        # Keep going until done\n",
    "        while True:\n",
    "            steps += 1\n",
    "            actions = {}\n",
    "\n",
    "            # Get the action of each agent\n",
    "            for agent in env.agents:\n",
    "                policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                # Ensure that the policy is normalized\n",
    "                #policy = torch.softmax(policy, dim=-1)\n",
    "\n",
    "                # Obtain the action from the policy\n",
    "                action = torch.multinomial(policy, num_samples=1)\n",
    "                actions[agent.name] = action\n",
    "\n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            for agent in env.agents:\n",
    "                # Convert action to tensor with one-hot encoding\n",
    "                action_tensor = nn.functional.one_hot(actions[agent.name], num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "                # Compute the intrinsic reward\n",
    "                _, predicted_next_state = self.icm(obs[agent.name], next_obs[agent.name], action_tensor)\n",
    "                intrinsic_reward = self.icm.mse_loss(predicted_next_state, next_obs[agent.name])\n",
    "\n",
    "                # Compute the extrinsic reward\n",
    "                extrinsic_reward = rewards[agent.name]\n",
    "\n",
    "                # Combine the intrinsic and extrinsic rewards\n",
    "                comb_rew = extrinsic_reward + intrinsic_reward * self.intrinsic_reward_weight\n",
    "\n",
    "                # Store the rollouts\n",
    "                rollouts[agent.name].append((obs[agent.name], actions[agent.name], comb_rew, next_obs[agent.name], dones))\n",
    "\n",
    "            obs = next_obs\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        return rollouts, steps\n",
    "    \n",
    "    def train_icm(self, rollouts):\n",
    "        \"\"\"\n",
    "        Trains the curiosity model using the rollouts previously collected\n",
    "        \"\"\"\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, actions, _, next_obs, _ = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs = torch.tensor(np.array(obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            next_obs = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device).squeeze(1)\n",
    "            \n",
    "            actions = nn.functional.one_hot(torch.tensor(actions), num_classes=self.action_size).float().squeeze(1).to(device)\n",
    "\n",
    "            # Train the curiosity model\n",
    "            predicted_actions, predicted_next_state = self.icm(obs, next_obs, actions)\n",
    "\n",
    "            # Inverse loss\n",
    "            inverse_loss = nn.CrossEntropyLoss()(predicted_actions, actions)\n",
    "\n",
    "            # Forward loss\n",
    "            forward_loss = self.icm.mse_loss(predicted_next_state, next_obs)\n",
    "\n",
    "            # Total loss\n",
    "            loss = inverse_loss + forward_loss * 0.2\n",
    "\n",
    "            # Backpropagate\n",
    "            self.icm.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.icm.optimizer.step()\n",
    "\n",
    "    def get_advantages(self, rollouts):\n",
    "        \"\"\"\n",
    "        Computes the advantages for each agent using Generalized Advantage Estimation from the rollouts previously collected\n",
    "        \"\"\"\n",
    "        advantages = {agent: [] for agent in rollouts.keys()}\n",
    "        for agent in rollouts.keys():\n",
    "            # Obtain the rollouts of each individual agent\n",
    "            agent_rollouts = rollouts[agent]\n",
    "            obs, _, rewards, next_obs, dones = zip(*agent_rollouts)\n",
    "\n",
    "            # Convert to tensors\n",
    "            obs_tensor = torch.tensor(np.array(obs), dtype=torch.float32, device=device)\n",
    "            next_obs_tensor = torch.tensor(np.array(next_obs), dtype=torch.float32, device=device)\n",
    "            dones_tensor = torch.tensor(np.array(dones), dtype=torch.float32, device=device)\n",
    "\n",
    "            # Get the values from the critic\n",
    "            _, values = self.shared_model(obs_tensor)\n",
    "            _, next_values = self.shared_model(next_obs_tensor)\n",
    "\n",
    "            # Remove extra dimensions\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "\n",
    "            # Compute the advantages\n",
    "            returns = []\n",
    "            advantage = 0.0\n",
    "            for reward, value, next_value, done in zip(reversed(rewards), reversed(values), reversed(next_values), reversed(dones_tensor)):\n",
    "                # GAE\n",
    "                td_error = reward + (1 - done) * self.gamma * next_value - value\n",
    "                advantage = td_error + self.gamma * self.lmbda * advantage\n",
    "                returns.insert(0, advantage + value)  # GAE + baseline\n",
    "\n",
    "            advantages[agent] = torch.tensor(returns, dtype=torch.float32, device=device) - values\n",
    "        return advantages\n",
    "\n",
    "    def ppo_update(self, rollouts, advantages, num_iters=5):\n",
    "        all_obs, all_actions, all_adv = [], [], []\n",
    "\n",
    "        # Collect all observations, actions and returns\n",
    "        for agent in rollouts.keys():\n",
    "            obs, actions, _, _, _ = zip(*rollouts[agent])\n",
    "            all_obs.extend(obs)\n",
    "            all_actions.extend(actions)\n",
    "            all_adv.extend(advantages[agent])\n",
    "\n",
    "        # Convert to tensors\n",
    "        obs = torch.tensor(np.array(all_obs), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(all_actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "        adv = torch.tensor(all_adv, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Train the ICM module\n",
    "        self.train_icm(rollouts)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        for _ in range(num_iters):\n",
    "            # Forward pass the Network\n",
    "                policy, values = self.shared_model(obs)\n",
    "                old_policy = policy.detach()\n",
    "                policy = torch.softmax(policy, dim=-1) # Normalize policy\n",
    "\n",
    "                # Remove extra dimension from policy to match actions\n",
    "                policy = policy.squeeze(1)\n",
    "                old_policy = old_policy.squeeze(1)\n",
    "\n",
    "                # Compute action probabilities for policy and old policy\n",
    "                policy_probs = policy.gather(1, actions)  # Select action probabilities\n",
    "                old_policy_probs = old_policy.gather(1, actions)  # Select old action probabilities\n",
    "\n",
    "                # Compute the ratio of probabilities\n",
    "                ratios = (policy_probs / old_policy_probs).squeeze()  # Remove extra dimension\n",
    "\n",
    "                print(f\"Ratios mean: {ratios.mean()}\")\n",
    "\n",
    "                # Policy Loss\n",
    "                clip = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "                policy_loss = -torch.min(ratios * adv, clip * adv).mean()\n",
    "\n",
    "                # Value loss\n",
    "                returns = adv + values.squeeze().detach()\n",
    "                value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                #value_loss = nn.MSELoss()(values.squeeze(), adv + values.squeeze())\n",
    "\n",
    "                # Entropy loss\n",
    "                entropy_loss = -(policy * torch.log(policy + 1e-8)).sum(dim=-1).mean()\n",
    "                self.entropy_coeff = max(self.entropy_coeff * 0.995, 0.01)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + 0.5 * value_loss - self.entropy_coeff * entropy_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.shared_model.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.shared_model.parameters(), max_norm=1.0)\n",
    "\n",
    "                self.shared_model.optimizer.step()\n",
    "\n",
    "    def plot_rewards(self, episode, max_episodes, history, show_result=False):\n",
    "        plt.figure(1)\n",
    "\n",
    "        if show_result:\n",
    "            plt.title(f'Final Result:')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title(f\"Episode {episode} of {max_episodes}\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps taken')\n",
    "\n",
    "        plt.plot(history)\n",
    "\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Trains the agents using Proximal Policy Optimization\n",
    "        \"\"\"\n",
    "\n",
    "        step_history = []\n",
    "        plt.ion()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            rollouts, steps = self.collect_rollout(self.env)\n",
    "            advantages = self.get_advantages(rollouts)\n",
    "            self.ppo_update(rollouts, advantages)\n",
    "\n",
    "            step_history.append(steps)\n",
    "\n",
    "            self.plot_rewards(episode, num_episodes, step_history)\n",
    "\n",
    "            #print(f\"Episode {episode + 1} with {steps} steps\", end=\"\\r\", flush=True)\n",
    "        \n",
    "        self.plot_rewards(episode, num_episodes, step_history, show_result=True)\n",
    "        plt.ioff()\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Tests the trained agents using the environment and renders the environment\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            obs = self.env.reset()\n",
    "            dones = [False]\n",
    "            while not dones[0]:\n",
    "                actions = {}\n",
    "                # Get the action of each agent\n",
    "                for agent in env.agents:\n",
    "                    policy, _ = self.shared_model(obs[agent.name])\n",
    "\n",
    "                    # Obtain the action from the policy\n",
    "                    action = torch.multinomial(policy, num_samples=1)\n",
    "                    actions[agent.name] = action\n",
    "                \n",
    "                obs, _, dones, _ = self.env.step(actions)\n",
    "\n",
    "                self.env.render(mode=\"human\")\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Runs the interactive environment\n",
    "        \"\"\"\n",
    "        vmas.render_interactively(scenario_name, display_info=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(env, observation_size, action_size)\n",
    "ppo.train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(ppo.shared_model.state_dict(), \"models/ppo_model.pth\")\n",
    "torch.save(ppo.icm.state_dict(), \"models/icm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ppo.shared_model.load_state_dict(torch.load(\"ppo_model.pth\"))\n",
    "ppo.icm.load_state_dict(torch.load(\"icm_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
